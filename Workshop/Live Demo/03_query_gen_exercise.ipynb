{"cells":[{"cell_type":"markdown","metadata":{"id":"51DicgTK9_Jm"},"source":["# Synthetic Query Generation\n","\n","Synthetic data generation has become a crucial technique in AI development, especially when working with specialized domains where obtaining high-quality human-labeled data is challenging.\n","\n","As highlighted in a recent [Answer.AI blog post](https://www.answer.ai/posts/2024-10-15-how-to-synthesize-data.html), the key to effective synthetic data lies in balancing two critical factors:\n","\n","1. **Quality** - Ensuring the generated queries are accurate, relevant, and useful\n","2. **Diversity** - Creating a wide range of query types, styles, and perspectives\n","\n","If you're interested in synthetic data generation in general, we highly recommend to check out this blog post. We will implement a similar process targeted at query generation in this notebook.\n","\n","In this notebook, we will get to the main topic of this workshop: *synthetic* ***query*** *generation*.\n","\n","By the end of this notebook, you'll have a comprehensive understanding of how to generate high-quality, diverse synthetic queries for fine-tuning embedding models on specialized domains."]},{"cell_type":"markdown","source":["## Setup\n","\n","> ***Important:*** *As we won't need it in this notebook and usage is limited, make sure you are* ***not*** *using a GPU runtime. Click on `Runtime` > `Change runtime type` > Select `CPU` and Save.*\n","\n","> *Also, to make sure there are no older sessions running, click on `Runtime` > `Manage sessions` > `Terminate other sessions`*\n","\n","We will use the same setup as in notebook `01_intro.ipynb`"],"metadata":{"id":"5_u2nHKyBD1A"}},{"cell_type":"code","source":["!wget \"https://drive.google.com/uc?export=download&id=1kTbWY9JJf0fFoqZGh6d-DRHQel6sT-9Y\" -O ./sample_data.csv\n","!wget \"https://drive.google.com/uc?export=download&id=1hBGWmXKW2LhMZ9rOd05UTg_aUp_nJ_Wt\" -O ./fewshot_examples.csv"],"metadata":{"id":"UStKYu5PFXeo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745757619202,"user_tz":-120,"elapsed":4847,"user":{"displayName":"dataclair","userId":"08395925427596862417"}},"outputId":"7ace7a47-a9e9-4ea8-9c1f-78a801fe16d4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-04-27 12:40:14--  https://drive.google.com/uc?export=download&id=1kTbWY9JJf0fFoqZGh6d-DRHQel6sT-9Y\n","Resolving drive.google.com (drive.google.com)... 172.253.117.100, 172.253.117.101, 172.253.117.138, ...\n","Connecting to drive.google.com (drive.google.com)|172.253.117.100|:443... connected.\n","HTTP request sent, awaiting response... 303 See Other\n","Location: https://drive.usercontent.google.com/download?id=1kTbWY9JJf0fFoqZGh6d-DRHQel6sT-9Y&export=download [following]\n","--2025-04-27 12:40:14--  https://drive.usercontent.google.com/download?id=1kTbWY9JJf0fFoqZGh6d-DRHQel6sT-9Y&export=download\n","Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 74.125.199.132, 2607:f8b0:400e:c02::84\n","Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|74.125.199.132|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 88246 (86K) [application/octet-stream]\n","Saving to: ‘./sample_data.csv’\n","\n","./sample_data.csv   100%[===================>]  86.18K  --.-KB/s    in 0.001s  \n","\n","2025-04-27 12:40:16 (79.1 MB/s) - ‘./sample_data.csv’ saved [88246/88246]\n","\n","--2025-04-27 12:40:16--  https://drive.google.com/uc?export=download&id=1hBGWmXKW2LhMZ9rOd05UTg_aUp_nJ_Wt\n","Resolving drive.google.com (drive.google.com)... 172.253.117.100, 172.253.117.101, 172.253.117.138, ...\n","Connecting to drive.google.com (drive.google.com)|172.253.117.100|:443... connected.\n","HTTP request sent, awaiting response... 303 See Other\n","Location: https://drive.usercontent.google.com/download?id=1hBGWmXKW2LhMZ9rOd05UTg_aUp_nJ_Wt&export=download [following]\n","--2025-04-27 12:40:16--  https://drive.usercontent.google.com/download?id=1hBGWmXKW2LhMZ9rOd05UTg_aUp_nJ_Wt&export=download\n","Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 74.125.199.132, 2607:f8b0:400e:c02::84\n","Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|74.125.199.132|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 7224 (7.1K) [application/octet-stream]\n","Saving to: ‘./fewshot_examples.csv’\n","\n","./fewshot_examples. 100%[===================>]   7.05K  --.-KB/s    in 0s      \n","\n","2025-04-27 12:40:19 (60.6 MB/s) - ‘./fewshot_examples.csv’ saved [7224/7224]\n","\n"]}]},{"cell_type":"code","source":["import os\n","import random\n","import time\n","from typing import Dict, Any, Optional, List\n","from google import genai\n","from google.colab import userdata\n","from google.genai import types\n","from IPython.display import display, Markdown\n","\n","os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")  # alternatively paste your key here\n","client = genai.Client(api_key=os.getenv(\"GOOGLE_API_KEY\"))"],"metadata":{"id":"ARAIRApDBJsZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_text(\n","    prompt: str,\n","    model: str = \"gemini-2.0-flash\",\n","    temperature: Optional[float] = None,\n","    max_tokens: Optional[int] = None,\n","    system_instructions: Optional[str] = None\n",") -> str:\n","    \"\"\"\n","    Generate text using Google's Gemini model with configurable parameters.\n","\n","    Args:\n","        prompt: The user prompt to send to the model\n","        model: Model name to use (default: gemini-2.0-flash)\n","        temperature: Controls temperature (0.0-2.0, lower is more deterministic)\n","        max_tokens: Maximum number of tokens to generate\n","        system_instructions: Optional system instruction to guide the model\n","\n","    Returns:\n","        Generated text response as string\n","    \"\"\"\n","    try:\n","        # Create config with only non-None parameters\n","        config_params = {}\n","        if temperature:\n","            config_params[\"temperature\"] = temperature\n","        if max_tokens is not None:\n","            config_params[\"max_output_tokens\"] = max_tokens\n","        if system_instructions:\n","            config_params[\"system_instruction\"] = system_instructions\n","\n","        # Create the config object\n","        config = types.GenerateContentConfig(**config_params)\n","\n","        # Generate content\n","        response = client.models.generate_content(\n","            model=model,\n","            contents=[prompt],\n","            config=config\n","        )\n","\n","        return response.text\n","    except Exception as e:\n","        return f\"Error generating text: {str(e)}\""],"metadata":{"id":"0SjRvwCSBK2b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BAPN7QDX9_Jm"},"source":["## 1. Basic Query Generation\n","\n","Let's start with the most basic approach to generating synthetic queries: simply asking an LLM to generate questions about the EU AI Act. This method requires minimal setup and can quickly produce a set of queries to serve as starting point.\n","\n","We'll use Gemini to generate some initial queries about the EU AI Act without providing any specific context from the actual documents.\n","\n","**Exercise 1a:**\n","> Write a short system prompt that tells Gemini more about the application we're building."]},{"cell_type":"code","metadata":{"id":"vkJ143MK9_Jm"},"source":["system_prompt = \"\"\"describe in 1-3 sentences what we're trying to achieve\"\"\""],"outputs":[],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"9MIU0yC_9_Jn"},"source":["**Exercise 1b:**\n","> Now write a simple prompt that asks the LLM to generate a question about the EU's AI Act. Then call Gemini with this prompt as well as the system prompt you wrote above. Note that you might need to add an instruction to tell it only to return the query and nothing else."]},{"cell_type":"code","metadata":{"id":"ERnej-j-9_Jn"},"source":["basic_prompt = \"\"\"write your prompt here\"\"\"\n","\n","# Generate the questions\n","response = ...\n","print(response)"],"outputs":[],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"Wn2PRWcP9_Jn"},"source":["**Exercise 1c:**\n","> Now run the same prompt 5 times"]},{"cell_type":"code","metadata":{"id":"iimjVgSE9_Jn"},"source":["..."],"outputs":[],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"-w_Fizna9_Jn"},"source":["**Reflection:**\n","> How would you evaluate the quality of these queries?"]},{"cell_type":"markdown","metadata":{"id":"rpqqwgJ39_Jo"},"source":["## 2. Grounded Generation\n","\n","To address the limitations we observed above, we'll now explore a more effective approach: grounding our query generation in actual passages from the EU AI Act.\n","\n","This technique significantly improves both the quality and diversity of our synthetic queries by:\n","1. Ensuring queries are relevant to the actual content of the document\n","2. Naturally increasing diversity as different passages cover different aspects of the legislation\n","3. Incorporating accurate terminology and concepts from the source material\n","\n","Let's load a few random passages from the AI Act."]},{"cell_type":"code","metadata":{"id":"Q573pLWx9_Jo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745757624599,"user_tz":-120,"elapsed":1475,"user":{"displayName":"dataclair","userId":"08395925427596862417"}},"outputId":"565d09c7-f6b2-4bf5-85a9-f55becb36421"},"source":["import pandas as pd\n","df = pd.read_csv(\"sample_data.csv\")\n","len(df)"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["100"]},"metadata":{},"execution_count":4}],"execution_count":null},{"cell_type":"code","metadata":{"id":"tm5jw9d49_Jo"},"source":["passages = df.sample(5, random_state=10)[\"passage\"].values.tolist()"],"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["Print out one passage which we'll use for grounded generation"],"metadata":{"id":"zSTTgdRwNvj5"}},{"cell_type":"code","metadata":{"id":"5cZWtWAz9_Jo","colab":{"base_uri":"https://localhost:8080/","height":300},"executionInfo":{"status":"ok","timestamp":1745757624632,"user_tz":-120,"elapsed":24,"user":{"displayName":"dataclair","userId":"08395925427596862417"}},"outputId":"8c1070e5-8d66-4d44-ba5f-b6129734e47b"},"source":["passage = passages[0]\n","display(Markdown(passage))"],"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Chapter XII - PENALTIES\n\nArticle 99 - Penalties\n\n7.   When deciding whether to impose an administrative fine and when deciding on the amount of the administrative fine in each individual case, all relevant circumstances of the specific situation shall be taken into account and, as appropriate, regard shall be given to the following: (a) the nature, gravity and duration of the infringement and of its consequences, taking into account the purpose of the AI system, as well as, where appropriate, the number of affected persons and the level of damage suffered by them; (b) whether administrative fines have already been applied by other market surveillance authorities to the same operator for the same infringement; (c) whether administrative fines have already been applied by other authorities to the same operator for infringements of other Union or national law, when such infringements result from the same activity or omission constituting a relevant infringement of this Regulation; (d) the size, the annual turnover and market share of the operator committing the infringement; (e) any other aggravating or mitigating factor applicable to the circumstances of the case, such as financial benefits gained, or losses avoided, directly or indirectly, from the infringement; (f) the degree of cooperation with the national competent authorities, in order to remedy the infringement and mitigate the possible adverse effects of the infringement; (g) the degree of responsibility of the operator taking into account the technical and organisational measures implemented by it; (h) the manner in which the infringement became known to the national competent authorities, in particular whether, and if so to what extent, the operator notified the infringement; (i) the intentional or negligent character of the infringement; (j) any action taken by the operator to mitigate the harm suffered by the affected persons."},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"ja8Svh_69_Jo"},"source":["In addition to the passage, we will also provide the LLM with a list of criteria which define a high quality query.\n","\n","**Exercise 2a:**\n","> Write a list of criteria which define a high quality query. For example, a high quality query should be specific, relevant and answerable by the passage."]},{"cell_type":"code","metadata":{"id":"_PInyody9_Jp"},"source":["criteria = \"\"\"\n","1. criterion 1\n","2. criterion 2\n","...\n","\"\"\""],"outputs":[],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"VbkoBV7Y9_Jp"},"source":["**Exercise 2b:**\n","> Write a prompt that asks the LLM to generate a query based on the provided criteria and passage.\n","\n","<details>\n","<summary>Click to see a hint</summary>\n","\n","Add the passage using a placeholder:\n","\n","```\n","\"\"\"\n","Write a query about the following passage:\n","\n","## Passage\n","\n","{passage}\n","\n","## Criteria\n","\n","Follow these criteria:\n","1. criterion 1\n","2. criterion 2\n","...\n","\"\"\"\n","```\n","\n","</details>"]},{"cell_type":"code","metadata":{"id":"sXToQBU89_Jp"},"source":["grounded_prompt = \"\"\"write your prompt here\"\"\""],"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["**Tip:**\n","> Always print out the fully formatted prompt to make sure everything is correct\n","\n","<details>\n","<summary>Click to see a hint</summary>\n","\n","Format the prompt by passing a passage to it:\n","\n","```\n","prompt = grounded_prompt.format(passage=passage)\n","```\n","\n","</details>"],"metadata":{"id":"ER983Qf-N52t"}},{"cell_type":"code","metadata":{"id":"oBJEsE-59_Jp"},"source":["prompt = ...\n","print(prompt)"],"outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"id":"AWMVKmeg9_Jp"},"source":["response = ...\n","print(response)"],"outputs":[],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"74GF27RR9_Jp"},"source":["**Exercise 2c:**\n","> Call Gemini 5 times with this prompt, each time using a different passage.\n","\n","**Tip:**\n","> *Look at your data!* Always make sure to print out the context used for grounding, along with the generated query. Only if you can see all the relevant information yourself will you be able to judge a query's quality."]},{"cell_type":"code","metadata":{"id":"r_q1NyMA9_Jx"},"source":["..."],"outputs":[],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"oE_gr2JV9_Jy"},"source":["**Reflection:**\n","> What do you think? Did this technique improve the generated questions?"]},{"cell_type":"markdown","metadata":{"id":"KyhWnQ149_Jy"},"source":["**Exercise 2d:**\n","> Now call Gemini 5 times with this prompt, each time using the same passage."]},{"cell_type":"code","metadata":{"id":"b7ZI--oq9_Jy"},"source":["..."],"outputs":[],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"QQ-pV9qI9_Jz"},"source":["## 3. Persona-Based Generation\n","\n","Our previous approach successfully grounded queries in relevant passages, but we still observed limited diversity in query styles and perspectives. To address this limitation, we'll now explore persona-based generation, a powerful technique for further enhancing the diversity of our synthetic queries.\n","\n","Persona-based generation involves creating detailed character profiles (personas) that guide the LLM to generate content from specific perspectives. This approach was introduced in the paper \"Scaling Synthetic Data Creation with 1,000,000,000 Personas\".\n","\n","The key benefits of persona-based generation for our query task include:\n","\n","1. **Stylistic diversity** - Different personas use different language patterns, terminology, and complexity levels\n","2. **Varied perspectives** - Personas with different backgrounds approach topics with unique concerns and priorities\n","3. **Realistic variation** - Real users come from diverse backgrounds and have different levels of domain knowledge\n","\n","Let's implement this approach by creating a set of personas with varying backgrounds, knowledge levels, and interests in the EU AI Act."]},{"cell_type":"markdown","metadata":{"id":"q52d4H8K9_Jz"},"source":["**Exercise 3a:**\n","> Create a list of 5 diverse personas who might have questions about the EU AI Act. Consider including different professions, technical backgrounds, and reasons for interest in the legislation."]},{"cell_type":"code","metadata":{"id":"XaV29c7Q9_J0"},"source":["personas = [\n","    \"A data protection officer at a large European enterprise implementing AI compliance\",\n","    \"A software developer specializing in machine learning applications\",\n","    ...\n","]"],"outputs":[],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"FWhAXVVL9_J0"},"source":["In addition to personas, we will also will generate a few different query styles to further increase diversity.\n","\n","**Exercise 3b:**\n","> Create a list of 5 different query styles. Try to make them as realistic and diverse as possible."]},{"cell_type":"code","metadata":{"id":"jwactKWS9_J0"},"source":["query_styles = [\n","    \"Technical language with domain-specific terminology\",\n","    \"Simple direct question with basic vocabulary\",\n","    ...\n","]"],"outputs":[],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"I6XEF6N59_J0"},"source":["**Exercise 3c:**\n","> Enhance our prompt from part 2 to also include persona and query style. Then call Gemini once with this prompt based on a single persona and query style.\n","\n","<details>\n","<summary>Click to see a hint</summary>\n","\n","Structure your prompt like this:\n","\n","```\n","\"\"\"\n","Write a query based on a passage, persona and query style.\n","\n","## Passage\n","\n","{passage}\n","\n","## Context:\n","\n","Persona: {persona}\n","Query style: {query_style}\n","\n","## Criteria\n","\n","Follow these criteria:\n","1. criterion 1\n","2. criterion 2\n","...\n","\"\"\"\n","```\n","\n","</details>"]},{"cell_type":"code","metadata":{"id":"SEzJIZ3a9_J0"},"source":["persona_prompt = \"\"\"write your prompt here\"\"\""],"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["<details>\n","<summary>Click to see a hint</summary>\n","\n","Pass all necessary inputs to your prompt:\n","\n","```\n","prompt = persona_prompt.format(\n","    passage=passage, persona=personas[0], query_style=query_styles[0]\n",")\n","```\n","\n","</details>"],"metadata":{"id":"FkmonHZjC7YE"}},{"cell_type":"code","metadata":{"id":"16Gr-aSX9_J0"},"source":["prompt = ...\n","print(prompt)"],"outputs":[],"execution_count":null},{"cell_type":"code","source":["response = ...\n","print(response)"],"metadata":{"id":"mcHgRt1qge9m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eap9obfN9_J0"},"source":["**Exercise 3d:**\n","> Run Gemini 5 times with this prompt on the same passage, but sample a different persona and query style each time you run it.\n","\n","<details>\n","<summary>Click to see a hint</summary>\n","\n","Sample a random item using `random.choice`:\n","\n","```\n","persona = random.choice(personas)\n","```"]},{"cell_type":"code","metadata":{"id":"jLaXZAm29_J1"},"source":["..."],"outputs":[],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"wPjOqTog9_J1"},"source":["**Reflect:**\n","> What do you think? Are we already happy with the diversity and quality of the queries?"]},{"cell_type":"markdown","metadata":{"id":"Z0FRWGVM9_J1"},"source":["## 4. Few-Shot Generation\n","\n","So far, we've made significant progress in generating diverse, relevant queries using passage grounding and persona-based techniques. Our queries are now much more varied in style, complexity, and perspective. However, we're still relying heavily on the LLM to interpret our instructions, personas, and query styles correctly.\n","\n","To gain more control over the generation process and further improve quality, we implement few-shot learning. This technique involves providing the LLM with carefully curated examples of exactly what we want it to produce and is frequently used in papers such as InPars (2022), Promptagator (2022), SWIM-IR (2024) and Gecko (2024).\n","\n","By showing the model high-quality examples that demonstrate the desired output format, style, and quality, we can:\n","1. **Increase consistency** - Examples provide concrete guidance on expected output format and quality\n","2. **Improve adherence to criteria** - Seeing examples helps the model better understand our quality criteria\n","3. **Reduce misinterpretations** - Examples clarify how personas and query styles should be applied\n","4. **Raise the quality bar** - Well-crafted examples set a higher standard for the generated queries\n","\n","Let's enhance our prompt with a few carefully selected examples that demonstrate the kind of high-quality, diverse queries we want to generate."]},{"cell_type":"markdown","metadata":{"id":"ZpGHX5jV9_J1"},"source":["**Load example data**\n","\n","Load the 4 few-shot examples we have prepared. For your own use case, you can either write few-shot examples by hand or generate them with the help of an LLM. However, make sure to review them carefully and if necessary, filter or improve them to ensure high quality.\n","\n","*Note that you need to provide not only example queries, but all additional inputs to the prompt, such as the passage, persona, and query style.*"]},{"cell_type":"code","metadata":{"id":"4672oL8E9_J1","colab":{"base_uri":"https://localhost:8080/","height":174},"executionInfo":{"status":"ok","timestamp":1745757634423,"user_tz":-120,"elapsed":65,"user":{"displayName":"dataclair","userId":"08395925427596862417"}},"outputId":"62c6d890-ef90-4acb-9367-8985e20d8eef"},"source":["dfe = pd.read_csv(\"fewshot_examples.csv\")\n","dfe.head()"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                             passage  \\\n","0  Chapter IX - POST-MARKET MONITORING, INFORMATI...   \n","1  Preamble\\n\\n(174)Given the rapid technological...   \n","2  Chapter II - PROHIBITED AI PRACTICES\\n\\nArticl...   \n","3  ANNEX IV\\n\\n(b) the design specifications of t...   \n","\n","                                             persona  \\\n","0  A legal consultant who specializes in technolo...   \n","1  A journalist who covers technology trends for ...   \n","2  A municipal government official responsible fo...   \n","3  A software developer specializing in machine l...   \n","\n","                                         query_style  \\\n","0  Technical language with domain-specific termin...   \n","1       Simple direct question with basic vocabulary   \n","2  Search engine keyword query without full sente...   \n","3  Informal conversational question with filler w...   \n","\n","                                               query  \n","0  What authority do EU regulatory bodies have to...  \n","1  How often will the EU evaluate if their AI reg...  \n","2  prohibited AI social scoring systems governmen...  \n","3  Hey, so um, what kind of documentation do I ne...  "],"text/html":["\n","  <div id=\"df-d035ca93-fd62-480d-8aab-acda89f3a831\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>passage</th>\n","      <th>persona</th>\n","      <th>query_style</th>\n","      <th>query</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Chapter IX - POST-MARKET MONITORING, INFORMATI...</td>\n","      <td>A legal consultant who specializes in technolo...</td>\n","      <td>Technical language with domain-specific termin...</td>\n","      <td>What authority do EU regulatory bodies have to...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Preamble\\n\\n(174)Given the rapid technological...</td>\n","      <td>A journalist who covers technology trends for ...</td>\n","      <td>Simple direct question with basic vocabulary</td>\n","      <td>How often will the EU evaluate if their AI reg...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Chapter II - PROHIBITED AI PRACTICES\\n\\nArticl...</td>\n","      <td>A municipal government official responsible fo...</td>\n","      <td>Search engine keyword query without full sente...</td>\n","      <td>prohibited AI social scoring systems governmen...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>ANNEX IV\\n\\n(b) the design specifications of t...</td>\n","      <td>A software developer specializing in machine l...</td>\n","      <td>Informal conversational question with filler w...</td>\n","      <td>Hey, so um, what kind of documentation do I ne...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d035ca93-fd62-480d-8aab-acda89f3a831')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-d035ca93-fd62-480d-8aab-acda89f3a831 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-d035ca93-fd62-480d-8aab-acda89f3a831');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-9db2dadc-6dbe-4404-9cbc-47556a3729b5\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9db2dadc-6dbe-4404-9cbc-47556a3729b5')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-9db2dadc-6dbe-4404-9cbc-47556a3729b5 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"dfe","summary":"{\n  \"name\": \"dfe\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"passage\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Preamble\\n\\n(174)Given the rapid technological developments and the technical expertise required to effectively apply this Regulation, the Commission should evaluate and review this Regulation by 2\\u00a0August 2029 and every four years thereafter and report to the European Parliament and the Council. In addition, taking into account the implications for the scope of this Regulation, the Commission should carry out an assessment of the need to amend the list of high-risk AI systems and the list of prohibited practices once a\\u00a0year. Moreover, by 2\\u00a0August 2028 and every four years thereafter, the Commission should evaluate and report to the European Parliament and to the Council on the need to amend the list of high-risk areas headings in the annex to this Regulation, the AI systems within the scope of the transparency obligations, the effectiveness of the supervision and governance system and the progress on the development of standardisation deliverables on energy efficient development of general-purpose AI models, including the need for further measures or actions. Finally, by 2\\u00a0August 2028 and every three years thereafter, the Commission should evaluate the impact and effectiveness of voluntary codes of conduct to foster the application of the requirements provided for high-risk AI systems in the case of AI systems other than high-risk AI systems and possibly other additional requirements for such AI systems.\",\n          \"ANNEX\\u00a0IV\\n\\n(b) the design specifications of the system, namely the general logic of the AI system and of the algorithms; the key design choices including the rationale and assumptions made, including with regard to persons or groups of persons in respect of who, the system is intended to be used; the main classification choices; what the system is designed to optimise for, and the relevance of the different parameters; the description of the expected output and output quality of the system; the decisions about any possible trade-off made regarding the technical solutions adopted to comply with the requirements set out in Chapter\\u00a0III, Section\\u00a02;\\n(c) the description of the system architecture explaining how software components build on or feed into each other and integrate into the overall processing; the computational resources used to develop, train, test and validate the AI system;\\n(d) where relevant, the data requirements in terms of datasheets describing the training methodologies and techniques and the training data sets used, including a\\u00a0general description of these data sets, information about their provenance, scope and main characteristics; how the data was obtained and selected; labelling procedures (e.g. for supervised learning), data cleaning methodologies (e.g. outliers detection);\\n(e) assessment of the human oversight measures needed in accordance with Article\\u00a014, including an assessment of the technical measures needed to facilitate the interpretation of the outputs of AI systems by the deployers, in accordance with Article\\u00a013(3), point (d);\",\n          \"Chapter IX - POST-MARKET MONITORING, INFORMATION SHARING AND MARKET SURVEILLANCE\\n\\nSection 3 - Enforcement\\n\\nArticle 77 - Powers of authorities protecting fundamental rights\\n\\n3.\\u00a0\\u00a0\\u00a0Where the documentation referred to in paragraph\\u00a01 is insufficient to ascertain whether an infringement of obligations under Union law protecting fundamental rights has occurred, the public authority or body referred to in paragraph\\u00a01 may make a\\u00a0reasoned request to the market surveillance authority, to organise testing of the high-risk AI system through technical means. The market surveillance authority shall organise the testing with the close involvement of the requesting public authority or body within a\\u00a0reasonable time following the request.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"persona\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"A journalist who covers technology trends for a mainstream news outlet\",\n          \"A software developer specializing in machine learning applications\",\n          \"A legal consultant who specializes in technology and intellectual property law\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"query_style\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Simple direct question with basic vocabulary\",\n          \"Informal conversational question with filler words\",\n          \"Technical language with domain-specific terminology\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"query\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"How often will the EU evaluate if their AI regulations are successful?\",\n          \"Hey, so um, what kind of documentation do I need to provide about my training datasets when I'm submitting my AI system for compliance review?\",\n          \"What authority do EU regulatory bodies have to conduct technical testing of AI systems when investigating potential fundamental rights violations?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":7}],"execution_count":null},{"cell_type":"code","source":["def format_few_shot_examples(examples, k=None):\n","    \"\"\"\n","    Format a list of few-shot examples into a markdown-formatted string for prompts.\n","\n","    Args:\n","        examples: List of dictionaries containing 'passage', 'persona', 'query_style', and 'query'\n","        k: Optional number of examples to randomly sample (if None, use all examples)\n","\n","    Returns:\n","        A markdown-formatted string containing the examples\n","    \"\"\"\n","    # If k is specified, randomly sample k examples\n","    if k is not None and k < len(examples):\n","        examples = random.sample(list(examples), k)\n","\n","    formatted_examples = []\n","    for i, example in enumerate(examples):\n","        example_text = f\"## Example {i+1}\\n\\n\"\n","        example_text += f\"### Passage:\\n\\n{example['passage']}\\n\\n\"\n","        example_text += f\"### Context:\\n\\nPersona: {example['persona']}\\nQuery style: {example['query_style']}\\n\\n\"\n","        example_text += f\"### Generated Query: {example['query']}\"\n","        formatted_examples.append(example_text)\n","\n","    return \"\\n\\n\".join(formatted_examples)"],"metadata":{"id":"S3AckaEXNM8D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We format our few-shot examples as a single string using the function above."],"metadata":{"id":"769mDxvUDrll"}},{"cell_type":"code","source":["examples = dfe.to_dict(orient=\"records\")\n","examples_formatted = format_few_shot_examples(examples, k=3)\n","print(examples_formatted)"],"metadata":{"id":"5VQXj8q0Dqm2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745759080440,"user_tz":-120,"elapsed":51,"user":{"displayName":"dataclair","userId":"08395925427596862417"}},"outputId":"507d57ab-1927-4c2a-b6bc-bee43e149ffe"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["## Example 1\n","\n","### Passage:\n","\n","Chapter II - PROHIBITED AI PRACTICES\n","\n","Article 5 - Prohibited AI practices\n","\n","1.   The following AI practices shall be prohibited: (a) the placing on the market, the putting into service or the use of an AI system that deploys subliminal techniques beyond a person’s consciousness or purposefully manipulative or deceptive techniques, with the objective, or the effect of materially distorting the behaviour of a person or a group of persons by appreciably impairing their ability to make an informed decision, thereby causing them to take a decision that they would not have otherwise taken in a manner that causes or is reasonably likely to cause that person, another person or group of persons significant harm; (b) the placing on the market, the putting into service or the use of an AI system that exploits any of the vulnerabilities of a natural person or a specific group of persons due to their age, disability or a specific social or economic situation, with the objective, or the effect, of materially distorting the behaviour of that person or a person belonging to that group in a manner that causes or is reasonably likely to cause that person or another person significant harm; (c) the placing on the market, the putting into service or the use of AI systems for the evaluation or classification of natural persons or groups of persons over a certain period of time based on their social behaviour or known, inferred or predicted personal or personality characteristics, with the social score leading to either or both of the following: (i) detrimental or unfavourable treatment of certain natural persons or groups of persons in social contexts that are unrelated to the contexts in which the data was originally generated or collected; (ii) detrimental or unfavourable treatment of certain natural persons or groups of persons that is unjustified or disproportionate to their social behaviour or its gravity; (d) the placing on the market, the putting into service for this specific purpose, or the use of an AI system for making risk assessments of natural persons in order to assess or predict the risk of a natural person committing a criminal offence, based solely on the profiling of a natural person or on assessing their personality traits and characteristics; this prohibition shall not apply to AI systems used to support the human assessment of the involvement of a person in a criminal activity, which is already based on objective and verifiable\n","\n","### Context:\n","\n","Persona: A municipal government official responsible for digital transformation initiatives\n","Query style: Search engine keyword query without full sentence structure\n","\n","### Generated Query: prohibited AI social scoring systems government services EU law\n","\n","## Example 2\n","\n","### Passage:\n","\n","Preamble\n","\n","(174)Given the rapid technological developments and the technical expertise required to effectively apply this Regulation, the Commission should evaluate and review this Regulation by 2 August 2029 and every four years thereafter and report to the European Parliament and the Council. In addition, taking into account the implications for the scope of this Regulation, the Commission should carry out an assessment of the need to amend the list of high-risk AI systems and the list of prohibited practices once a year. Moreover, by 2 August 2028 and every four years thereafter, the Commission should evaluate and report to the European Parliament and to the Council on the need to amend the list of high-risk areas headings in the annex to this Regulation, the AI systems within the scope of the transparency obligations, the effectiveness of the supervision and governance system and the progress on the development of standardisation deliverables on energy efficient development of general-purpose AI models, including the need for further measures or actions. Finally, by 2 August 2028 and every three years thereafter, the Commission should evaluate the impact and effectiveness of voluntary codes of conduct to foster the application of the requirements provided for high-risk AI systems in the case of AI systems other than high-risk AI systems and possibly other additional requirements for such AI systems.\n","\n","### Context:\n","\n","Persona: A journalist who covers technology trends for a mainstream news outlet\n","Query style: Simple direct question with basic vocabulary\n","\n","### Generated Query: How often will the EU evaluate if their AI regulations are successful?\n","\n","## Example 3\n","\n","### Passage:\n","\n","ANNEX IV\n","\n","(b) the design specifications of the system, namely the general logic of the AI system and of the algorithms; the key design choices including the rationale and assumptions made, including with regard to persons or groups of persons in respect of who, the system is intended to be used; the main classification choices; what the system is designed to optimise for, and the relevance of the different parameters; the description of the expected output and output quality of the system; the decisions about any possible trade-off made regarding the technical solutions adopted to comply with the requirements set out in Chapter III, Section 2;\n","(c) the description of the system architecture explaining how software components build on or feed into each other and integrate into the overall processing; the computational resources used to develop, train, test and validate the AI system;\n","(d) where relevant, the data requirements in terms of datasheets describing the training methodologies and techniques and the training data sets used, including a general description of these data sets, information about their provenance, scope and main characteristics; how the data was obtained and selected; labelling procedures (e.g. for supervised learning), data cleaning methodologies (e.g. outliers detection);\n","(e) assessment of the human oversight measures needed in accordance with Article 14, including an assessment of the technical measures needed to facilitate the interpretation of the outputs of AI systems by the deployers, in accordance with Article 13(3), point (d);\n","\n","### Context:\n","\n","Persona: A software developer specializing in machine learning applications\n","Query style: Informal conversational question with filler words\n","\n","### Generated Query: Hey, so um, what kind of documentation do I need to provide about my training datasets when I'm submitting my AI system for compliance review?\n"]}]},{"cell_type":"markdown","metadata":{"id":"ysGceyFX9_J1"},"source":["**Exercise 4a:**\n","> Enhance our prompt from 3c to also include few shot examples. Print out the full final prompt and then call Gemini once with this prompt.\n","\n","**Tip:**\n","> You can further increase the diversity of the data generation process by randomly sampling a subset of examples."]},{"cell_type":"markdown","source":["<details>\n","<summary>Click to see a hint</summary>\n","\n","Structure your prompt like this:\n","\n","```\n","\"\"\"\n","Write a query based on a passage, persona and query style.\n","\n","# Criteria:\n","\n","Follow these criteria\n","1. criterion 1\n","2. criterion 2\n","...\n","\n","# Examples\n","\n","{examples_formatted}\n","\n","# Your task\n","\n","Now generate a query about the following passage.\n","\n","### Passage:\n","\n","{passage}\n","\n","### Context:\n","\n","Persona: {persona}\n","Query style: {query_style}\n","\n","### Generated Query:\"\"\"\n","```"],"metadata":{"id":"-PycHcbnADMg"}},{"cell_type":"code","metadata":{"id":"U1dKPXpq9_J2"},"source":["fewshot_prompt = \"\"\"write your prompt here\"\"\""],"outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"id":"4r7ENM9l9_J2"},"source":["prompt = ...\n","print(prompt)"],"outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"id":"0s3yLKYh9_J2"},"source":["response = ...\n","print(response)"],"outputs":[],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"RvdIPJoe9_J2"},"source":["**Exercise 4b:**\n","> As before, run 5 iterations with the same passage, and sample a different persona and query style each time you run it. However, at each iteration call both `persona_prompt` and `fewshot_prompt` so that we're able to directly compare them. Also, store the generated queries, along with their sampled personas and query styles in a list, as we'll use them in the next section."]},{"cell_type":"code","metadata":{"id":"SC7R_dv39_J2"},"source":["results = []\n","\n","print(\"# PASSAGE:\\n\")\n","display(Markdown(passage))\n","print(\"\\n\" + \"-\"*80 + \"\\n\")\n","\n","for i in range(5):\n","    # Sample a random persona and query style\n","    ...\n","\n","    # Format the prompts with the same passage, persona, and query style\n","    ...\n","\n","    # Generate responses from both prompts\n","    ...\n","    time.sleep(0.5)  # Avoid overloading the API\n","\n","    # Store results\n","    results.append({\n","        \"persona\": ...,\n","        \"query_style\": ...,\n","        \"persona_query\": ...,\n","        \"fewshot_query\": ...\n","    })\n","\n","    # Print results for comparison\n","    ..."],"outputs":[],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"AJE5CM489_J2"},"source":["**Reflect:**\n","> What do you notice about the queries generated by the two prompts? Which one do you think is better?"]},{"cell_type":"markdown","metadata":{"id":"KyqCqe-k9_J3"},"source":["## 5. Quality Filtering\n","\n","We've now explored several techniques for generating diverse, high-quality synthetic queries. By combining passage grounding, persona-based generation, and few-shot examples, we've significantly improved both the quality and diversity of our synthetic data. However, even with these advanced techniques, not every generated query will meet our standards.\n","\n","A crucial final step in synthetic data generation is quality filtering. As highlighted in the Answer.AI blog:\n","\n","> \"To address these concerns, let’s use another prompt. It will evaluate and filter the generations. We’ll use the 5-point scoring system in The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale. It proved most effective at evaluating the quality of data.\"\n","\n","The FineWeb paper introduced an additive scoring approach where points are accumulated based on satisfying specific quality criteria. The LLM is instructed to first write a critique of the generated example and then score it based on the provided scoring system."]},{"cell_type":"markdown","metadata":{"id":"mfazNnx79_J3"},"source":["Here is the prompt used in the blog post:"]},{"cell_type":"code","metadata":{"id":"U-CHcp2-9_J3"},"source":["eval_prompt_template = \"\"\"\n","Below is an extract of a translation. Evaluate its quality as a senior translator would, considering its suitability for professional use. Use the additive 5-point scoring system described below. Points are accumulated based on the satisfaction of each criterion:\n","\n","- Add 1 point if the translation conveys the basic meaning of the source text, even if it includes some minor errors or awkward phrasing.\n","- Add another point if the translation is generally accurate but lacks refinement in style or fails to capture some nuances of the original. It might use inconsistent terminology or have occasional lapses in register.\n","- Award a third point if the translation is appropriate for professional use and accurately conveys key concepts of the source text. It demonstrates good understanding of both languages, though it may not be flawless or could include some slight inconsistencies. It resembles the work of a competent translator but may have room for improvement in fluency or precision.\n","- Grant a fourth point if the translation is highly accurate and reads naturally in the target language, exhibiting a consistent and appropriate style. It could be similar to the work of an experienced translator, offering faithful rendering of content and tone, with minimal errors, and effectively handling complex concepts or cultural references. The result is coherent, well-expressed, and valuable for its intended purpose.\n","- Bestow a fifth point if the translation is outstanding, demonstrating mastery of both source and target languages. It captures subtle nuances, maintains the author's voice and intent, and reads as if it were originally written in the target language. The translator has made excellent choices in dealing with challenging elements like wordplay, idiomatic expressions, or culture-specific content.\n","\n","<translation>\n","{translation}\n","</translation>\n","\n","After examining the translation:\n","\n","- Briefly justify your total score in a single line.\n","- Conclude with the score of the translation.\"\"\""],"outputs":[],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"cMCSbBIZ9_J3"},"source":["**Exercise 5a:**\n","> Rewrite the scoring instructions above to fit our use case. Reuse the quality criteria you have written before. We already prefilled all the rest for you.\n","\n","**Tip:**\n","> Asking the model to return its reponse in JSON format will simplify parsing and postprocessing."]},{"cell_type":"code","metadata":{"id":"SWTyy5Of9_J3"},"source":["quality_filter_prompt = \"\"\"\n","Write your scoring instructions here\n","\n","## Passage:\n","\n","{passage}\n","\n","## Context:\n","Persona: {persona}\n","Query style: {query_style}\n","\n","## Generated Query: {query}\n","\n","After examining the query:\n","\n","- Briefly justify your total score in a single line.\n","- Conclude with the score of the query (1-5).\n","\n","Return the evaluation in valid JSON format with double quotes:\n","\n","Evaluation = {{\"critique\": str, \"score\": int}}\n","Return: Evaluation\n","\"\"\""],"outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"id":"q1vaYM1E9_J3"},"source":["prompt = ...\n","print(prompt)"],"outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"id":"EyuDTMQT9_J3"},"source":["response = ...\n","print(response)"],"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["We can use this function to parse the response"],"metadata":{"id":"bH2oePomOGoO"}},{"cell_type":"code","source":["import json\n","\n","def extract_json(response_text):\n","    \"\"\"Extract and parse JSON from LLM response text, handling various formats.\"\"\"\n","    try:\n","        # First try to extract JSON from markdown code blocks if present\n","        if \"```\" in response_text:\n","            # Extract content between code blocks\n","            json_text = response_text.split(\"```\")[1]\n","            # Remove language indicator if present\n","            if json_text.startswith(\"json\"):\n","                json_text = json_text[4:].strip()\n","        else:\n","            json_text = response_text.strip()\n","\n","        # Parse the JSON\n","        return json.loads(json_text)\n","    except (json.JSONDecodeError, IndexError) as e:\n","        print(f\"Error parsing JSON: {e}\")\n","        print(f\"Raw response: {response_text}\")\n","        # Return a default value in case of error\n","        return {\"critique\": \"Error parsing response\", \"score\": 0}"],"metadata":{"id":"Z6RYxaglOF-F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["extract_json(response)"],"metadata":{"id":"LGkXefC7crnj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bAKs0WAh9_J3"},"source":["**Exercise 5b:**\n","> Now run the prompt on all results you stored in the previous section, parse the responses and store the critiques and scores together with the previous results. To simplify things we already wrote this part for you."]},{"cell_type":"code","metadata":{"id":"G_pqGeUr9_J4"},"source":["# Print the passage so we can easily check it\n","print(\"# PASSAGE:\\n\")\n","display(Markdown(passage))\n","print(\"\\n\" + \"-\"*80 + \"\\n\")\n","\n","# Evaluate all queries and store results\n","for i, result in enumerate(results):\n","    print(f\"Evaluating query {i+1}/5...\\n\")\n","\n","    # Evaluate persona-based query\n","    persona_filter_prompt = quality_filter_prompt.format(\n","        passage=passage,\n","        persona=result[\"persona\"],\n","        query_style=result[\"query_style\"],\n","        query=result[\"persona_query\"]\n","    )\n","    persona_response = generate_text(persona_filter_prompt, system_instructions=system_prompt)\n","\n","    # Evaluate few-shot query\n","    fewshot_filter_prompt = quality_filter_prompt.format(\n","        passage=passage,\n","        persona=result[\"persona\"],\n","        query_style=result[\"query_style\"],\n","        query=result[\"fewshot_query\"]\n","    )\n","    fewshot_response = generate_text(fewshot_filter_prompt, system_instructions=system_prompt)\n","    time.sleep(0.5)\n","\n","    # Parse and store evaluations\n","    try:\n","        persona_eval = extract_json(persona_response)\n","        fewshot_eval = extract_json(fewshot_response)\n","\n","        # Add evaluations to results\n","        results[i][\"persona_critique\"] = persona_eval.get(\"critique\", \"Error\")\n","        results[i][\"persona_score\"] = persona_eval.get(\"score\", 0)\n","        results[i][\"fewshot_critique\"] = fewshot_eval.get(\"critique\", \"Error\")\n","        results[i][\"fewshot_score\"] = fewshot_eval.get(\"score\", 0)\n","\n","        # Print personas and query styles\n","        print(f\"  PERSONA: {results[i]['persona']}\")\n","        print(f\"  QUERY STYLE: {results[i]['query_style']}\")\n","\n","        # Print scores during generation\n","        print(f\"\\n  ZERO-SHOT SCORE: {results[i]['persona_score']}\")\n","        print(f\"     Query: {results[i]['persona_query'].strip()}\")\n","        print(f\"     Critique: {results[i]['persona_critique'].strip()}\")\n","        print(f\"\\n  FEW-SHOT SCORE: {results[i]['fewshot_score']}\")\n","        print(f\"     Query: {results[i]['fewshot_query'].strip()}\")\n","        print(f\"     Critique: {results[i]['fewshot_critique'].strip()}\")\n","\n","    except Exception as e:\n","        print(f\"Error processing result {i}: {e}\")\n","\n","    print(\"\\n\" + \"-\"*80 + \"\\n\")\n","\n","# Calculate average scores\n","avg_persona_score = sum(r.get(\"persona_score\", 0) for r in results) / len(results)\n","avg_fewshot_score = sum(r.get(\"fewshot_score\", 0) for r in results) / len(results)\n","\n","print(\"=\" * 80)\n","print(\"AVERAGE SCORES:\")\n","print(f\"Zero-shot queries: {avg_persona_score:.2f}\")\n","print(f\"Few-shot queries: {avg_fewshot_score:.2f}\")\n","print(\"=\" * 80)"],"outputs":[],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"szch5joi9_J5"},"source":["## Conclusion\n","\n","In this notebook, we've explored a comprehensive approach to synthetic query generation for specialized domains like the EU AI Act. We've progressed from basic generation to increasingly sophisticated techniques:\n","\n","1. **Basic generation** demonstrated the limitations of simple prompting\n","2. **Grounded generation** improved relevance by anchoring queries to specific passages\n","3. **Persona-based generation** enhanced diversity through varied perspectives and styles\n","4. **Few-shot learning** provided more control over output quality and format\n","5. **Quality filtering** ensured only the best queries make it into our final dataset\n","\n","These techniques allow us to create synthetic queries that are both high-quality and diverse.\n","\n","However, generating good queries is only the first step in creating a robust training dataset for embedding models. In the next notebook, we'll explore the crucial process of mining positive and negative examples - identifying which passages truly answer our queries and which ones don't. This step is essential for creating the clean, well-structured training data needed for effective embedding model fine-tuning."]}],"metadata":{"kernelspec":{"display_name":"python3","language":"python","name":"python3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}