{"cells":[{"cell_type":"markdown","metadata":{"id":"yJsiaciOz6AU"},"source":["# Introduction to Embeddings and RAG\n","\n","This notebook provides a concise introduction to text embeddings, retrieval, and retrieval-augmented generation (RAG). It serves as a foundation for the main focus of our workshop: **Synthetic Data Generation for Embedding Model Fine-Tuning**.\n","\n","Regardless of your experience level with these concepts, this introduction will equip you with the essential knowledge needed for the hands-on exercises that follow. If you're new to the topic, don't worry if some details seem complex at first—you'll still be able to follow along with the workshop's core activities.\n","\n","By the end of this notebook, you'll understand the fundamental concepts that underpin our work with synthetic data generation and embedding model fine-tuning."]},{"cell_type":"markdown","metadata":{"id":"9IBLEOihz6AU"},"source":["## Setup\n","\n","> ***Important:*** *As we won't need it in this notebook and usage is limited, make sure you are* ***not*** *using a GPU runtime. Click on `Runtime` > `Change runtime type` > Select `CPU` and Save.*\n","\n","> *Also, to make sure there are no older sessions running, click on `Runtime` > `Manage sessions` > `Terminate other sessions`*\n","\n","We strongly recommend to use your own Gemini API key during this workshop. See notebook `01_intro.ipynb`\n","\n","However, if you cannot use your own Google account, we provide a backup key using *Gemini-2.0-Flash* through LiteLLM. They key we provide is budget-limited and will expire after the workshop. Do not use it for anything else than the exercises in this workshop.\n","\n","Go to Google Colab's `Secrets` and add the following two secrets (we will provide the values separately):\n","- `LITELLM_API_KEY`\n","- `LITELLM_API_BASE`\n","\n","Make sure `Notebook access` is enabled for both.\n"]},{"cell_type":"code","source":["!pip install litellm"],"metadata":{"id":"7xwbzjgPMiau"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uDuuY-egz6AU"},"source":["import os\n","import litellm\n","from typing import Dict, Any, Optional, List\n","from google.colab import userdata\n","from IPython.display import display, Markdown\n","\n","litellm.api_key = userdata.get(\"LITELLM_API_KEY\")\n","litellm.api_base = userdata.get(\"LITELLM_API_BASE\")"],"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["For embeddings, we will use a local model through the `Sentence Transformers` library."],"metadata":{"id":"RyRQe2uWOBHY"}},{"cell_type":"code","source":["from sentence_transformers import SentenceTransformer\n","model = SentenceTransformer(\"all-MiniLM-L12-v2\")"],"metadata":{"id":"SrSfjI9zN9t6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zsM0gi1hz6AV"},"source":["We define two helper functions `generate_text` and `embed_text` so that we quickly and conveniently can call the Gemini text generation and embedding endpoints"]},{"cell_type":"code","source":["def generate_text(\n","    prompt: str,\n","    model: str = \"gemini-2.0-flash-001\",\n","    temperature: Optional[float] = None,\n","    max_tokens: Optional[int] = None,\n","    system_instructions: Optional[str] = None\n",") -> str:\n","    \"\"\"\n","    Generate text using Google's Gemini model with configurable parameters.\n","\n","    Args:\n","        prompt: The user prompt to send to the model\n","        model: Model name to use (only supports gemini-2.0-flash-001)\n","        temperature: Controls temperature (0.0-2.0, lower is more deterministic)\n","        max_tokens: Maximum number of tokens to generate\n","        system_instructions: Optional system instruction to guide the model\n","\n","    Returns:\n","        Generated text response as string\n","    \"\"\"\n","    try:\n","        # Prepare messages\n","        messages = []\n","        if system_instructions:\n","            messages.append({\"role\": \"system\", \"content\": system_instructions})\n","        messages.append({\"role\": \"user\", \"content\": prompt})\n","\n","        # Generate content\n","        response = litellm.completion(\n","            model=\"openai/\"+model,\n","            messages=messages,\n","            temperature=temperature,\n","            max_tokens=max_tokens\n","        )\n","\n","        return response.choices[0].message.content\n","    except Exception as e:\n","        return f\"Error generating text: {str(e)}\"\n","\n","def embed_text(\n","    text: str,\n",") -> List[float]:\n","    \"\"\"\n","    Generate embeddings for a text string.\n","\n","    Args:\n","        text: The text to generate embeddings for\n","\n","    Returns:\n","        List of embedding values as floats\n","    \"\"\"\n","    try:\n","        # Generate embeddings\n","        embedding = model.encode(text)\n","\n","        # Return the embedding values\n","        return embedding.tolist()\n","    except Exception as e:\n","        print(f\"Error generating embedding: {str(e)}\")\n","        return []"],"metadata":{"id":"B9_QDvNZMAR8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BCmlJaOTz6AV"},"source":["## Part 1 - Case study: EU AI Act\n","\n","Imagine this hypothetical scenario: You work as a Machine Learning Engineer at a hot new startup which builds AI solutions in the legal domain. You were tasked to build the model for a new system that should help users of any kind (lawyers, journalists, policy makers, companies..) answer questions about the EU's Artificial Intelligence Act (or short, AI Act): https://en.wikipedia.org/wiki/Artificial_Intelligence_Act\n","\n","If you need a quick refresher yourself, the AI Act is a comprehensive regulation establishing a common legal framework for artificial intelligence within the EU. It aims to ensure that AI systems used within the EU are safe, transparent, and respect fundamental rights. The Act came into force on August 1, 2024, with its provisions being implemented gradually over the following 6 to 36 months.\n","\n","For your model to be a considered success, it needs to:\n","1. accurately answer the user's questions\n","2. be based on the latest and official version of the AI Act\n","3. be able to directly copy passages from the regulatory text\n","4. to provide references to specific sections and articles\n","\n","One potential client of your new system sent you an example question that they, as a security department at a large enterprise working on AI models, have: *'How does the reach of a general-purpose AI model to thousands of business users in the EU affect its classification as having systemic risk?'*\n","\n","With LLMs recently getting better and better at generating text you're thinking to try out the easist way first, simply throwing the question into an LLM and see how well it answers the question. Let's go ahead and do that"]},{"cell_type":"code","metadata":{"id":"ACzJ9z1Oz6AV"},"source":["query = \"How does the reach of a general-purpose AI model to thousands of business users in the EU affect its classification as having systemic risk?\""],"outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"id":"YjND-em2z6AW"},"source":["answer = generate_text(query, system_instructions=\"Your task is to answer questions about the EU AI Act as accurately and concisely as possible.\")\n","display(Markdown(answer))"],"outputs":[],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"iZ50lNuWz6AW"},"source":["Alright, this answer sounds pretty good. But let's see if it is actually correct."]},{"cell_type":"code","metadata":{"id":"QrhkEqJKz6AW"},"source":["passage = \"\"\"ANNEX XIII\n","\n","Criteria for the designation of general-purpose AI models with systemic risk referred to in Article 51\n","For the purpose of determining that a general-purpose AI model has capabilities or an impact equivalent to those set out in Article 51(1), point (a), the Commission shall take into account the following criteria:\n","(a) the number of parameters of the model;\n","(b) the quality or size of the data set, for example measured through tokens;\n","(c) the amount of computation used for training the model, measured in floating point operations or indicated by a combination of other variables such as estimated cost of training, estimated time required for the training, or estimated energy consumption for the training;\n","(d) the input and output modalities of the model, such as text to text (large language models), text to image, multi-modality, and the state of the art thresholds for determining high-impact capabilities for each modality, and the specific type of inputs and outputs (e.g. biological sequences);\n","(e) the benchmarks and evaluations of capabilities of the model, including considering the number of tasks without additional training, adaptability to learn new, distinct tasks, its level of autonomy and scalability, the tools it has access to;\n","(f) whether it has a high impact on the internal market due to its reach, which shall be presumed when it has been made available to at least 10 000 registered business users established in the Union;\n","(g) the number of registered end-users.\"\"\""],"outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"id":"yIySIojFz6AW"},"source":["prompt = f\"\"\"You are a professional fact-checker and your task is to check the accuracy of an answer to a question. The accuracy of the answer should be assessed based on a provided passage from the EU AI Act.\n","\n","## Question:\n","{query}\n","\n","## Passage:\n","{passage}\n","\n","## Answer:\n","{answer}\n","\n","Point out any discrepancies you can find. Be thorough in your assessment and provide a concise answer.\"\"\""],"outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"id":"EGsihGecz6AW"},"source":["display(Markdown(generate_text(prompt)))"],"outputs":[],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"Vnk3318az6AW"},"source":["So while the answer is actually quite good, it is not 100% accurate, which is a problem for your customers as they require an extremely high degree of reliability from your system.\n","\n","Also, remember that they want the model to quote from the regulartory text."]},{"cell_type":"code","metadata":{"id":"unpiUyvVz6AW"},"source":["system_prompt = \"\"\"Your task is to answer questions about the EU AI Act as accurately and concisely as possible.\n","\n","In addition to your answer, provide:\n","- a direct quote of relevant sentences and\n","- mention in which part (section, article, paragraph) of the regulatory framework the sentence appeared.\"\"\"\n","\n","answer = generate_text(query, system_instructions=system_prompt)\n","display(Markdown(answer))"],"outputs":[],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"D8HzlPyWz6AW"},"source":["Now we have a problem. It seems like the model just entirely made this quote and article up.\n","\n","That's not the model's fault, as it has been a) optimized to be extremely helpful, so it even tries to \"help\" you when it can't and b) it doesn't know what it doesn't know.\n","\n","The risky thing here is that is sounds like the model knows what it's talking about and it generally even has a good grasp on the AI Act. However, for applications that require very high accuracy and reliablity, just asking an LLM for an answer is *simply not a good solution*."]},{"cell_type":"code","metadata":{"id":"0_5FB2Oyz6AX"},"source":["system_prompt = f\"\"\"Your task is to answer questions about the EU AI Act as accurately and concisely as possible.\n","\n","In addition to your answer, provide:\n","- a direct quote of relevant sentences and\n","- mention in which part (section, article, paragraph) of the regulatory framework the sentence appeared.\n","\n","Answer the question based on the following context:\n","{passage}\"\"\"\n","\n","answer = generate_text(query, system_instructions=system_prompt)\n","display(Markdown(answer))"],"outputs":[],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"R_3AyaByz6AX"},"source":["Based on the passage, the model can now easily answer the question, quote a sentence from the passage and provide a reference for double-checking.\n","\n","As you can see ***RAG is all about providing the right context to your LLM at the right time***\n","\n","Here we cheated of course, as we knew in advance which passage to provide to the model. In real life however, we need to find the right context out of a whole corpus of texts. And we need to be able to do that in a dynamic and efficient way.\n","\n","That brings us to the next part and main focus of this workshop: *How do we find the right context given a query?*"]},{"cell_type":"markdown","metadata":{"id":"qcAP10xLz6AX"},"source":["## Part 2 - Embeddings\n","\n","### What are Text Embeddings?\n","\n","Text embeddings are numerical representations of text in a high-dimensional vector space. They capture semantic meaning in a way that computers can process mathematically.\n","\n","Think of embeddings as translating words or sentences into a list of numbers (typically hundreds or thousands of dimensions) where:\n","- Similar texts are positioned close together in this space\n","- Dissimilar texts are positioned far apart\n","- The relationships between concepts are preserved\n","\n","We will start by embedding our query and passage into numerical vectors using the Gemini embedding API"]},{"cell_type":"code","metadata":{"id":"vInLcG-7z6AX"},"source":["query_vec = embed_text(query)\n","passage_vec = embed_text(passage)"],"outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"id":"hOcpB-0dz6AX"},"source":["len(query), len(query_vec)"],"outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"id":"rgKl9PVPz6AX"},"source":["len(passage), len(passage_vec)"],"outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"id":"MK8w80Enz6AX"},"source":["print(query_vec[:10])"],"outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"id":"A9CKSkqNz6AX"},"source":["print(passage_vec[:10])"],"outputs":[],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"K6xdLvmQz6AX"},"source":["The Gemini embeddings converted each input text into a 768-dimensional vector of values which we can use for all kinds of mathematical operations.\n","\n","Let's first embed another unrelated sentence."]},{"cell_type":"code","metadata":{"id":"YI-sGS0fz6AX"},"source":["passage2 = \"Prague is home to the world's oldest functioning astronomical clock, the Prague Orloj, which was installed in 1410. It is the third-oldest astronomical clock in the world and the oldest one still in operation. Every hour, a small show takes place where mechanical figures, including the Twelve Apostles, emerge and move around the clock!\"\n","passage2_vec = embed_text(passage2)"],"outputs":[],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"5tA1gFFzz6AX"},"source":["### Calculating Distance Between Embeddings\n","\n","To determine how semantically similar two texts are, we need to calculate the distance between their embedding vectors. The most common similarity measure for embeddings is **cosine similarity**.\n","\n","Cosine similarity measures the cosine of the angle between two vectors, effectively comparing their orientations rather than magnitudes. The formula is:\n","\n","$$\\text{cosine_similarity}(A, B) = \\frac{A \\cdot B}{||A|| \\: ||B||} = \\frac{\\sum_{i=1}^{n} A_i B_i}{\\sqrt{\\sum_{i=1}^{n} A_i^2} \\: \\sqrt{\\sum_{i=1}^{n} B_i^2}}$$\n","\n","Where:\n","- $A \\cdot B$ is the dot product of vectors A and B\n","- $||A||$ and $||B||$ are the magnitudes (L2 norms) of vectors A and B\n","\n","The result ranges from -1 (completely opposite) to 1 (exactly the same), with 0 indicating orthogonality (no similarity).\n","\n","Let's implement this from scratch using NumPy to understand what's happening under the hood:"]},{"cell_type":"code","metadata":{"id":"KvgmqE_ez6AY"},"source":["import numpy as np\n","\n","# Convert vectors to numpy arrays if they aren't already\n","vec_a = np.array(query_vec)\n","vec_b = np.array(passage_vec)"],"outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"id":"05Y3q4m8z6AY"},"source":["# Step 1: Calculate dot product (numerator)\n","dot_product = np.dot(vec_a, vec_b)\n","print(f\"Dot product: {dot_product}\")"],"outputs":[],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"E0TTPGq6z6AY"},"source":["The dot product measures similarity between vectors by calculating how much they point in the same direction.\n","\n","Geometrically, the dot product is defined as:\n","A·B = |A|×|B|×cos(θ)\n","\n","Where:\n","- |A| and |B| are the magnitudes of the vectors\n","- θ is the angle between them\n","- cos(θ) ranges from -1 (opposite directions) to 1 (same direction)\n","\n","When vectors point in similar directions, their dot product is larger. When they're perpendicular, it's zero. When they point in opposite directions, it's negative.\n","\n","For embedding vectors specifically, a higher dot product suggests the texts share similar semantic meaning. However, the raw dot product is affected by vector magnitudes, which is why we normalize it in cosine similarity to focus purely on direction."]},{"cell_type":"code","metadata":{"id":"lgP2PwSEz6AY"},"source":["# Step 2: Calculate L2 norms (magnitudes) of both vectors\n","norm_a = np.linalg.norm(vec_a)\n","norm_b = np.linalg.norm(vec_b)\n","print(f\"Norm of vector A: {norm_a}\")\n","print(f\"Norm of vector B: {norm_b}\")"],"outputs":[],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"O9QuOTZwz6AY"},"source":["The magnitude (or norm) of a vector represents its \"length\" in the vector space. For embedding vectors:\n","\n","- The magnitude tells us how \"strong\" the signal is in the embedding space\n","- Most embedding models normalize their outputs to have a magnitude close to 1.0 (unit vectors)\n","- This normalization makes comparing vectors more consistent, focusing on direction rather than magnitude\n","\n","In our example, both vectors have magnitudes very close to 1.0 (0.999...), which indicates they've been normalized by the embedding model. This is typical for modern embedding systems as it simplifies similarity calculations."]},{"cell_type":"code","metadata":{"id":"EcwzA6rMz6AY"},"source":["# Step 3: Calculate cosine similarity\n","similarity = dot_product / (norm_a * norm_b)\n","print(f\"Cosine similarity: {similarity}\")"],"outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"id":"pYnIsbY_z6AY"},"source":["def cosine_similarity(vec_a, vec_b):\n","    \"\"\"\n","    Calculate the cosine similarity between two vectors.\n","\n","    Args:\n","        vec_a: First vector (list or numpy array)\n","        vec_b: Second vector (list or numpy array)\n","\n","    Returns:\n","        Cosine similarity score between -1 and 1\n","    \"\"\"\n","    # Convert to numpy arrays if they aren't already\n","    vec_a = np.array(vec_a)\n","    vec_b = np.array(vec_b)\n","\n","    # Calculate dot product\n","    dot_product = np.dot(vec_a, vec_b)\n","\n","    # Calculate L2 norms (magnitudes)\n","    norm_a = np.linalg.norm(vec_a)\n","    norm_b = np.linalg.norm(vec_b)\n","\n","    # Calculate cosine similarity\n","    similarity = dot_product / (norm_a * norm_b)\n","\n","    return similarity.item()"],"outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"id":"fAgq4LkTz6AY"},"source":["cosine_similarity(query_vec, passage2_vec)"],"outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"id":"CuhHguXKz6Ad"},"source":["cosine_similarity(passage_vec, passage2_vec)"],"outputs":[],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"GTpWGMdxz6Ad"},"source":["As we can see, the random passage about Prague is more distant from both our query and passage from the AI Act.\n","\n","Let's try a more related passage about another regulatory topic (GDPR)"]},{"cell_type":"code","metadata":{"id":"BPwxSWfaz6Ad"},"source":["passage3 = \"The General Data Protection Regulation (Regulation (EU) 2016/679),[1] abbreviated GDPR, is a European Union regulation on information privacy in the European Union (EU) and the European Economic Area (EEA). The GDPR is an important component of EU privacy law and human rights law, in particular Article 8(1) of the Charter of Fundamental Rights of the European Union.\"\n","passage3_vec = embed_text(passage3)\n","cosine_similarity(query_vec, passage3_vec)"],"outputs":[],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"Dx0SJqkaz6Ad"},"source":["We'll also check the similarity score of another passage from the AI Act."]},{"cell_type":"code","metadata":{"id":"2AYiSUhdz6Ad"},"source":["passage4 = \"\"\"Chapter IX - POST-MARKET MONITORING, INFORMATION SHARING AND MARKET SURVEILLANCE\n","\n","Section 5 - Supervision, investigation, enforcement and monitoring in respect of providers of general-purpose AI models\n","\n","Article 92 - Power to conduct evaluations\n","\n","5.   The providers of the general-purpose AI model concerned or its representative shall supply the information requested. In the case of legal persons, companies or firms, or where the provider has no legal personality, the persons authorised to represent them by law or by their statutes, shall provide the access requested on behalf of the provider of the general-purpose AI model concerned.\"\"\"\n","passage4_vec = embed_text(passage4)\n","cosine_similarity(query_vec, passage4_vec)"],"outputs":[],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"6JyMV-09z6Ad"},"source":["As we can see, the more related the documents are, the higher values of cosine similarity we observe."]},{"cell_type":"markdown","metadata":{"id":"Vc7w8PPHz6Ad"},"source":["### Visualization\n","\n","To get a better understanding, we will also visualize the embeddings above in a two-dimensional space using dimensionality reduction so we can plot it."]},{"cell_type":"code","metadata":{"id":"-mvHKYF9z6Ad"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib.patches import FancyArrowPatch\n","from sklearn.decomposition import PCA"],"outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"id":"yINt_pNVz6Ae"},"source":["def visualize_embeddings(query_vec, passage_vecs, labels=None, title=\"Embedding Space Visualization\"):\n","    \"\"\"\n","    Visualize the distances between query and passages in 2D embedding space using sklearn's PCA.\n","\n","    Args:\n","        query_vec: Embedding vector of the query (list or numpy array)\n","        passage_vecs: List of embedding vectors for passages\n","        labels: List of labels for the passages (default: None)\n","        title: Title for the plot\n","\n","    Returns:\n","        List of similarities between query and passages\n","    \"\"\"\n","\n","    # Ensure all vectors are numpy arrays\n","    query_vec = np.array(query_vec)\n","    passage_vecs = [np.array(vec) for vec in passage_vecs]\n","\n","    # Create default labels if none provided\n","    if labels is None:\n","        labels = [f\"p{i+1}\" for i in range(len(passage_vecs))]\n","\n","    # Combine query and passages for dimensionality reduction\n","    all_vecs = np.vstack([query_vec] + passage_vecs)\n","\n","    # Calculate cosine similarities between query and each passage\n","    similarities = [cosine_similarity(query_vec, vec) for vec in passage_vecs]\n","\n","    # Reduce dimensionality to 2D using sklearn's PCA\n","    pca = PCA(n_components=2)\n","    vectors_2d = pca.fit_transform(all_vecs)\n","\n","    # Set up the plot\n","    plt.figure(figsize=(8, 6))  # Square figure\n","\n","    # Plot passages\n","    passage_points = plt.scatter(\n","        vectors_2d[1:, 0],\n","        vectors_2d[1:, 1],\n","        c=similarities,\n","        cmap='viridis',\n","        s=100,\n","        alpha=0.8\n","    )\n","\n","    # Plot query\n","    query_point = plt.scatter(\n","        vectors_2d[0, 0],\n","        vectors_2d[0, 1],\n","        c='red',\n","        s=150,\n","        marker='*',\n","        edgecolors='black'\n","    )\n","\n","    # Add colorbar to show similarity scale\n","    cbar = plt.colorbar(passage_points)\n","    cbar.set_label('Cosine Similarity to Query')\n","\n","    # Draw arrows from query to each passage\n","    for i in range(1, len(vectors_2d)):\n","        arrow = FancyArrowPatch(\n","            vectors_2d[0],\n","            vectors_2d[i],\n","            arrowstyle='->',\n","            alpha=0.3,\n","            mutation_scale=20\n","        )\n","        plt.gca().add_patch(arrow)\n","\n","    # Add labels\n","    plt.annotate(\n","        \"Query\",\n","        vectors_2d[0] + np.array([0.05, -0.05]),\n","        fontsize=8,\n","    )\n","\n","    # Add passage labels\n","    for i in range(len(labels)):\n","        plt.annotate(\n","            labels[i],\n","            vectors_2d[i+1] + np.array([0.05, 0.05]),\n","            fontsize=7,\n","        )\n","\n","    plt.title(title, fontsize=11)\n","    plt.grid(alpha=0.3)\n","\n","    # Set equal aspect ratio to ensure equal scaling\n","    plt.axis('equal')\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","    # Return the similarity scores\n","    return similarities\n"],"outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"id":"pGGbiaGsz6Ae"},"source":["visualize_embeddings(\n","    query_vec,\n","    [passage_vec, passage2_vec, passage3_vec, passage4_vec],\n","    [\"aiact_correct\", \"prague\", \"gdpr\", \"aiact_other\"]\n",")"],"outputs":[],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"d71z4wbFz6Ae"},"source":["This 2D visualization of our embedding space reveals some interesting insights:\n","\n","1. **Directional Clustering**: Notice how all three regulatory passages (aiact_correct, gdpr, aiact_other) point in roughly the same general direction from the query, forming a regulatory \"cluster\" in the embedding space. This demonstrates how embeddings capture domain relationships - texts about regulations share semantic similarities even when discussing different specific regulations.\n","\n","2. **Similarity Gradient**: The color intensity indicates cosine similarity to our query, with aiact_correct showing the strongest similarity (brightest yellow), followed by aiact_other (teal), then gdpr (darker blue). This aligns with our intuition - the passage specifically about GPAI systemic risk is most relevant to our query.\n","\n","3. **Unrelated Content**: The passage about Prague points in a completely different direction with the lowest similarity score (darkest purple), confirming that embeddings effectively separate unrelated semantic concepts.\n","\n","This visualization demonstrates why embedding-based retrieval works - relevant documents cluster in similar regions of the embedding space, allowing us to find semantically similar content even when the exact words don't match."]},{"cell_type":"markdown","metadata":{"id":"ZjX7J09wz6Ae"},"source":["## Part 3: Evaluation\n","\n","Next, we will learn how to evaluate text embedding or retrieval models given a query and a collection of documents.\n","\n","We will pick a slighly more challenging question from our dataset which has two passages labeled as relevant."]},{"cell_type":"code","metadata":{"id":"dyqbEtXiz6Ae"},"source":["query = \"Under what conditions does the AI Act exempt individual users from compliance when utilizing AI systems for personal, non-commercial purposes?\""],"outputs":[],"execution_count":null},{"cell_type":"code","source":["!wget -q \"https://drive.google.com/uc?export=download&id=1kTbWY9JJf0fFoqZGh6d-DRHQel6sT-9Y\" -O ./sample_data.csv"],"metadata":{"id":"Me0UbXKgymRm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","df = pd.read_csv(\"sample_data.csv\").iloc[:40]"],"metadata":{"id":"si-SKcJrKY5F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.head()"],"metadata":{"id":"CG8utVCozwz7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["passages = [(row.passage_id, row.passage) for row in df.itertuples(index=False)]"],"metadata":{"id":"___ExGXLKkgj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3nltrLJ8z6Ae"},"source":["Passages 0 and 1 are relevant for `query`, all others are irrelevant."]},{"cell_type":"code","source":["# prompt: group df by the value in column \"relevant\" and return counts\n","relevance_counts = df.groupby(\"relevant\").size()\n","relevance_counts"],"metadata":{"id":"2p0TfOXZ1ygq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zoCRAjBxz6Ae"},"source":["Generally speaking we want to evaluate ***retrieval accuracy***. In other words, we want to find out how well a model is able to find relevant passages for a given query from a large collection of documents which can range from hundreds to millions.\n","\n","The way we do this is the following:\n","1. embed query and all candidate documents\n","2. calculate cosine similarity between query and all candidate documents\n","3. rank documents by similarity score\n","4. calculate different metrics based on the ranking\n","\n","We will start by implementing steps 1-3"]},{"cell_type":"code","metadata":{"id":"f4NLzvOZz6Ae"},"source":["import time"],"outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"id":"1ayI1Wvuz6Ae"},"source":["# Step 1: Embed query and all candidate documents\n","passage_embeddings = []\n","for idx, passage in passages:\n","    try:\n","        embedding = embed_text(passage)\n","        passage_embeddings.append(embedding)\n","        time.sleep(0.2)  # avoid overloading the API\n","    except Exception as e:\n","        passage_embeddings.append(e)\n","errors = [e for e in passage_embeddings if isinstance(e, Exception)]\n","print(\"Number of errors:\", len(errors))"],"outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"id":"MlT-ab2Jz6Ae"},"source":["# Step 2: Calculate cosine similarity between query and all candidate documents\n","similarity_scores = []\n","for i, embedding in enumerate(passage_embeddings):\n","    score = cosine_similarity(query_vec, embedding)\n","    passage_idx = passages[i][0]\n","    similarity_scores.append((passage_idx, score))"],"outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"id":"bY1mMc9_z6Ae"},"source":["# Step 3: Rank documents by similarity score (descending order)\n","ranked_passages = sorted(similarity_scores, key=lambda x: x[1], reverse=True)"],"outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"id":"6SHIjeVJz6Ae"},"source":["[(f\"Rank {i+1}, ID {r[0]}, score {r[1]}\" + (\" (correct)\" if r[0] in [0,1] else \"\")) for i,r in enumerate(ranked_passages)]"],"outputs":[],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"_gchNS5tz6Ae"},"source":["### Basic retrieval metrics\n","\n","When evaluating embedding models for retrieval tasks, we use several metrics to measure performance from different angles. Each provides unique insights into how well our system finds relevant information.\n","\n","**Accuracy@k**: Measures whether at least one relevant document appears in the top k results.\n","- Binary metric: 1 if at least one relevant document is in top k, 0 otherwise\n","- Simple but doesn't account for ranking position or multiple relevant documents\n","- Formula: 1 if any relevant document in top k, else 0\n","\n","**Precision@k**: Measures the proportion of relevant documents among the top k retrieved documents.\n","- Formula: (Number of relevant documents in top k) / k\n","- Ranges from 0 to 1, with 1 meaning all top k documents are relevant\n","- Doesn't consider the ordering within the top k results\n","\n","**Recall@k**: Measures what fraction of all relevant documents are found in the top k results.\n","- Formula: (Number of relevant documents in top k) / (Total number of relevant documents)\n","- Ranges from 0 to 1, with 1 meaning all relevant documents are in top k\n","- Important when you need to find all relevant information"]},{"cell_type":"code","source":["def basic_retrieval_metrics(ranked_passages, relevant_ids, k=10):\n","    \"\"\"\n","    Calculate common retrieval metrics for a ranked list of passages.\n","\n","    Args:\n","        ranked_passages: List of tuples (passage_id, score) sorted by score in descending order\n","        relevant_ids: List of IDs that are considered relevant for the query\n","        k: The cutoff point for calculating metrics (default: 10)\n","\n","    Returns:\n","        Dictionary containing accuracy@k, precision@k, and recall@k\n","    \"\"\"\n","    # Ensure k is not larger than the number of ranked passages\n","    k = min(k, len(ranked_passages))\n","\n","    # Get the top k passage IDs\n","    top_k_ids = [passage_id for passage_id, _ in ranked_passages[:k]]\n","\n","    # Count relevant documents in top k\n","    relevant_in_top_k = sum(1 for passage_id in top_k_ids if passage_id in relevant_ids)\n","\n","    # Calculate metrics\n","    accuracy_at_k = 1 if relevant_in_top_k > 0 else 0\n","    precision_at_k = relevant_in_top_k / k if k > 0 else 0\n","    recall_at_k = relevant_in_top_k / len(relevant_ids) if relevant_ids else 0\n","\n","    return {\n","        f\"accuracy@{k}\": accuracy_at_k,\n","        f\"precision@{k}\": precision_at_k,\n","        f\"recall@{k}\": recall_at_k\n","    }\n"],"metadata":{"id":"2NtoKmtCMkal"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PvUs_r56z6Af"},"source":["basic_retrieval_metrics(ranked_passages, relevant_ids=[0,1], k=1)"],"outputs":[],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"iBYMyRz9z6Af"},"source":["At k=1 we didn't find any relevant document, hence all our metrics are zero."]},{"cell_type":"code","metadata":{"id":"bVEOajyiz6Af"},"source":["basic_retrieval_metrics(ranked_passages, relevant_ids=[0,1], k=5)"],"outputs":[],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"0hIb1qwSz6Af"},"source":["At k=5, we get:\n","- accuracy of 1 since we found a relevant passage\n","- precision of 0.2 as 1/5 passages was relevant\n","- recall of 0.5 as we found 1 out of two relevant passages in total"]},{"cell_type":"code","metadata":{"id":"D6TI849Sz6Af"},"source":["basic_retrieval_metrics(ranked_passages, relevant_ids=[0,1], k=25)"],"outputs":[],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"q-ART48rz6Af"},"source":["Only at k=25, we get:\n","- accuracy and recall of 1 as we found 2/2 relevant passages\n","- a low precision of 0.08 as most of the retrieved passages were irrelevant"]},{"cell_type":"markdown","metadata":{"id":"wdyJP5w6z6Af"},"source":["### Advanced Retrieval Metrics\n","\n","While basic metrics like Accuracy@k, Precision@k, and Recall@k provide a solid foundation for evaluating retrieval systems, advanced metrics offer more nuanced insights by considering the ranking positions of relevant documents and overall retrieval quality.\n","\n","<details>\n","<summary>Click to see explanation of MRR and MAP</summary>\n","\n","**Mean Reciprocal Rank (MRR)**: Measures the effectiveness of a retrieval system based on the rank of the first relevant document.\n","- Formula: MRR = 1/rank of first relevant document\n","- Ranges from 0 to 1, with higher values indicating better performance\n","- Focuses on the position of the first relevant result, which is often most important to users\n","- Example: If the first relevant document appears at rank 4, MRR = 1/4 = 0.25\n","\n","\n","Example for $k=3$:\n","* Accuracy@3 is 1 because there is a relevant document in top-3\n","* MRR@3 is 1/2 because the first relevant document is at the 2nd position\n","<img src=\"https://drive.google.com/uc?export=view&id=14DUQjmXAK1DxWcccL-fPZFScq89KEGnL\" alt=\"Evaluation metrics\" width=\"1000\">\n","\n","\n","**Mean Average Precision (MAP)**: provides a single-figure measure of quality across recall levels by averaging the precision values at positions where relevant documents are found.\n","\n","- Formula: MAP = average of precision@k for each k where a relevant document is retrieved\n","- Considers both precision and the order of relevant documents\n","- Penalizes systems that push relevant documents lower in the ranking\n","- Provides a more comprehensive view than single-point metrics\n","\n","Mean Average Precision across all queries\n","\n","- For each query, computes average precision (AP) as:\n","- $\\text{AP@}k = \\frac{\\sum_{i=1}^{k} P(i) \\times \\text{is_relevant}(i)}{\\text{min}(k, \\text{total relevant})}$\n","- Where $P(i)$ is precision at rank $i$, and is_relevant(i) is 1 if item at position $i$ is relevant else 0\n","- MAP is the mean of AP across all queries\n","\n","To compute the value for the whole dataset, we take the average over all queries.\n","</details>\n"]},{"cell_type":"markdown","source":["\n","**Normalized Discounted Cumulative Gain (nDCG)**: Measures the usefulness (gain) of retrieved documents based on their position in the result list, with the assumption that highly relevant documents appearing lower in the results list should be penalized.\n","\n","- Incorporates graded relevance (not just binary relevant/irrelevant)\n","- Discounts the value of documents that appear lower in the ranking\n","- Formula: DCG@k normalized by the \"ideal DCG\" (perfect ranking)\n","- Ranges from 0 to 1, with 1 indicating perfect ranking\n","- Particularly useful when documents have different degrees of relevance\n","\n","<details>\n","<summary>Click to see visual explanation of nDCG@k</summary>\n","This metric can be break down into multiple parts:\n","\n","- Cumulative gain (CG) @ $k$\n","    - sum of scores of all relevant items in top-k results\n","    - $\\sum_i^k r_i,$ where $r_i = 1$ the item at position $i$ is relevant and $0$ if it is not.\n","- Discounted CG (DCG) @ $k$\n","    - sum of scores divided by the logarithmic discount (log of the position) to give larger weight to results in top positions\n","    - $\\sum_i^k \\frac{r_i}{\\log_2(i+1)}$\n","- Normalized DCG (NDCG) @ $k$: divide computed DCG by ideal DCG (IDCG)\n","    - IDCG = all $n$ relevant documents are present in the top $n$ positions\n","$$ \\text{NDCG@}k = \\frac{\\text{DCG@}k}{\\text{IDCG@}k} $$\n","\n","<img src=\"https://drive.google.com/uc?export=view&id=10DXzOaweytqdYBTNCPWah0aN-i0pYA4-\" alt=\"Evaluation metrics\" width=\"1000\">\n","\n","So the NDCG@3 in this example is\n","\n","$$ \\text{NDCG@}k = \\frac{1.1}{2.1} = 0.52 $$\n","</details>"],"metadata":{"id":"xC0TZvh98-E9"}},{"cell_type":"code","metadata":{"id":"nFIBN1nUz6Af"},"source":["def mean_reciprocal_rank(ranked_passages, relevant_ids):\n","    \"\"\"\n","    Calculate Mean Reciprocal Rank (MRR).\n","\n","    Args:\n","        ranked_passages: List of tuples (passage_id, score) sorted by score in descending order\n","        relevant_ids: List of IDs that are considered relevant for the query\n","\n","    Returns:\n","        MRR value (float)\n","    \"\"\"\n","    # Extract passage IDs from ranked passages\n","    ranked_ids = [passage_id for passage_id, _ in ranked_passages]\n","\n","    # Find the rank of the first relevant document\n","    for i, passage_id in enumerate(ranked_ids):\n","        if passage_id in relevant_ids:\n","            return 1.0 / (i + 1)  # +1 because ranks start at 1, not 0\n","\n","    return 0.0  # No relevant documents found\n","\n","\n","def mean_average_precision(ranked_passages, relevant_ids):\n","    \"\"\"\n","    Calculate Mean Average Precision (MAP).\n","\n","    Args:\n","        ranked_passages: List of tuples (passage_id, score) sorted by score in descending order\n","        relevant_ids: List of IDs that are considered relevant for the query\n","\n","    Returns:\n","        MAP value (float)\n","    \"\"\"\n","    if not relevant_ids:\n","        return 0.0\n","\n","    # Extract passage IDs from ranked passages\n","    ranked_ids = [passage_id for passage_id, _ in ranked_passages]\n","\n","    relevant_count = 0\n","    sum_precision = 0.0\n","\n","    for i, passage_id in enumerate(ranked_ids):\n","        if passage_id in relevant_ids:\n","            relevant_count += 1\n","            # Precision at this point = relevant found so far / position\n","            precision_at_i = relevant_count / (i + 1)\n","            sum_precision += precision_at_i\n","\n","    # Average precision is the sum of precisions at relevant positions divided by total relevant docs\n","    return sum_precision / len(relevant_ids)\n","\n","\n","def normalized_dcg(ranked_passages, relevant_ids, k=None):\n","    \"\"\"\n","    Calculate Normalized Discounted Cumulative Gain (nDCG).\n","\n","    Args:\n","        ranked_passages: List of tuples (passage_id, score) sorted by score in descending order\n","        relevant_ids: List of IDs that are considered relevant for the query\n","        k: Optional cutoff point (if None, uses all passages)\n","\n","    Returns:\n","        nDCG value (float)\n","    \"\"\"\n","    if k is None:\n","        k = len(ranked_passages)\n","    else:\n","        k = min(k, len(ranked_passages))\n","\n","    # Extract passage IDs from ranked passages\n","    ranked_ids = [passage_id for passage_id, _ in ranked_passages][:k]\n","\n","    # For binary relevance, relevant documents have a gain of 1, irrelevant have 0\n","    gains = [1.0 if passage_id in relevant_ids else 0.0 for passage_id in ranked_ids]\n","\n","    # Calculate DCG (Discounted Cumulative Gain)\n","    dcg = gains[0] if gains else 0.0  # First element has no discount\n","    for i in range(1, len(gains)):\n","        # Use log base 2 for the discount\n","        dcg += gains[i] / np.log2(i + 2)  # +2 because log_2(2) = 1, and we want ranks to start at 1\n","\n","    # Calculate ideal DCG (IDCG) - create an ideal ranking with all relevant docs at the top\n","    num_relevant_to_consider = min(k, len(relevant_ids))\n","    ideal_gains = [1.0] * num_relevant_to_consider + [0.0] * (k - num_relevant_to_consider)\n","    ideal_gains = ideal_gains[:k]  # Ensure we only have k elements\n","\n","    idcg = ideal_gains[0] if ideal_gains else 0.0\n","    for i in range(1, len(ideal_gains)):\n","        idcg += ideal_gains[i] / np.log2(i + 2)\n","\n","    # Calculate nDCG\n","    return dcg / idcg if idcg > 0 else 0.0\n","\n","\n","def advanced_retrieval_metrics(ranked_passages, relevant_ids, k=None):\n","    \"\"\"\n","    Calculate all advanced retrieval metrics: MRR, MAP, and nDCG.\n","\n","    Args:\n","        ranked_passages: List of tuples (passage_id, score) sorted by score in descending order\n","        relevant_ids: List of IDs that are considered relevant for the query\n","        k: Optional cutoff point for nDCG calculation (if None, uses all passages)\n","\n","    Returns:\n","        Dictionary containing MRR, MAP, and nDCG@k values\n","    \"\"\"\n","    mrr = mean_reciprocal_rank(ranked_passages, relevant_ids)\n","    map_score = mean_average_precision(ranked_passages, relevant_ids)\n","    ndcg_score = normalized_dcg(ranked_passages, relevant_ids, k)\n","\n","    return {\n","        \"MRR\": mrr,\n","        \"MAP\": map_score,\n","        f\"nDCG@{k if k else len(ranked_passages)}\": ndcg_score.item()\n","    }"],"outputs":[],"execution_count":null},{"cell_type":"code","source":["advanced_retrieval_metrics(ranked_passages, relevant_ids=[0,1], k=5)"],"metadata":{"id":"7JUoBPjXxMWO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["advanced_retrieval_metrics(ranked_passages, relevant_ids=[0,1], k=25)"],"metadata":{"id":"YbQBxx4bxOCU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qls5o7QGz6Af"},"source":["**nDCG (0.264 at k=5, 0.395 at k=25)**: These values below 0.5 confirm that our ranking quality is mediocre. The increase from k=5 to k=25 suggests that our system does better at finding relevant documents when we look deeper into the results list, though the overall ranking quality remains suboptimal.\n","\n","<details>\n","<summary>Click to see the comments on MRR and MAP</summary>\n","\n","**MRR (0.25)**: This indicates that the first relevant document appears at position 4 (1/4 = 0.25). Even though our accuracy@5 is 1 (showing we found a relevant document), MRR reveals that this relevant document isn't at the top of our results.\n","\n","**MAP (0.165)**: This low MAP score reflects that our system isn't consistently ranking relevant documents highly. While we eventually find all relevant documents (recall=1 at k=25), the precision at each relevant document's position is suboptimal.\n","</details>\n","\n","These metrics demonstrate that while our system eventually finds all relevant documents (perfect recall at k=25), the *ranking* of these documents is problematic."]},{"cell_type":"markdown","metadata":{"id":"pdhwS2PLz6Af"},"source":["## Conclusion\n","\n","In this notebook, we learned about the fundamentals of text embeddings, retrieval, and evaluation. We started with a real-world case study of answering questions about the EU AI Act, demonstrating the limitations of directly using LLMs and the need for RAG.\n","\n","We covered the following:\n","- What are embeddings and how to generate them using Gemini\n","- How to calculate cosine similarity between embeddings\n","- How to visualize embeddings in a 2D space\n","- How to evaluate retrieval models using basic and advanced metrics\n","\n","Let's now move on to the next part where we dive deeper into embedding models and how they're trained."]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"colab":{"provenance":[{"file_id":"1cpIRtWp-gAzHVJewXKZZ8lxg0Q5hyFiC","timestamp":1745493450467}]}},"nbformat":4,"nbformat_minor":0}