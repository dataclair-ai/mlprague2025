{"cells":[{"cell_type":"markdown","metadata":{"id":"51DicgTK9_Jm"},"source":["# Synthetic Query Generation\n","\n","Synthetic data generation has become a crucial technique in AI development, especially when working with specialized domains where obtaining high-quality human-labeled data is challenging.\n","\n","As highlighted in a recent [Answer.AI blog post](https://www.answer.ai/posts/2024-10-15-how-to-synthesize-data.html), the key to effective synthetic data lies in balancing two critical factors:\n","\n","1. **Quality** - Ensuring the generated queries are accurate, relevant, and useful\n","2. **Diversity** - Creating a wide range of query types, styles, and perspectives\n","\n","If you're interested in synthetic data generation in general, we highly recommend to check out this blog post. We will implement a similar process targeted at query generation in this notebook.\n","\n","In this notebook, we will get to the main topic of this workshop: *synthetic* ***query*** *generation*.\n","\n","By the end of this notebook, you'll have a comprehensive understanding of how to generate high-quality, diverse synthetic queries for fine-tuning embedding models on specialized domains."]},{"cell_type":"markdown","source":["## Setup\n","\n","> ***Important:*** *As we won't need it in this notebook and usage is limited, make sure you are* ***not*** *using a GPU runtime. Click on `Runtime` > `Change runtime type` > Select `CPU` and Save.*\n","\n","> *Also, to make sure there are no older sessions running, click on `Runtime` > `Manage sessions` > `Terminate other sessions`*\n","\n","We will use the same setup as in notebook `01_intro.ipynb`"],"metadata":{"id":"5_u2nHKyBD1A"}},{"cell_type":"code","source":["!wget \"https://drive.google.com/uc?export=download&id=1kTbWY9JJf0fFoqZGh6d-DRHQel6sT-9Y\" -O ./sample_data.csv\n","!wget \"https://drive.google.com/uc?export=download&id=1hBGWmXKW2LhMZ9rOd05UTg_aUp_nJ_Wt\" -O ./fewshot_examples.csv"],"metadata":{"id":"UStKYu5PFXeo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import random\n","import time\n","from typing import Dict, Any, Optional, List\n","from google import genai\n","from google.colab import userdata\n","from google.genai import types\n","from IPython.display import display, Markdown\n","\n","os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")  # alternatively paste your key here\n","client = genai.Client(api_key=os.getenv(\"GOOGLE_API_KEY\"))"],"metadata":{"id":"ARAIRApDBJsZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_text(\n","    prompt: str,\n","    model: str = \"gemini-2.0-flash\",\n","    temperature: Optional[float] = None,\n","    max_tokens: Optional[int] = None,\n","    system_instructions: Optional[str] = None\n",") -> str:\n","    \"\"\"\n","    Generate text using Google's Gemini model with configurable parameters.\n","\n","    Args:\n","        prompt: The user prompt to send to the model\n","        model: Model name to use (default: gemini-2.0-flash)\n","        temperature: Controls temperature (0.0-2.0, lower is more deterministic)\n","        max_tokens: Maximum number of tokens to generate\n","        system_instructions: Optional system instruction to guide the model\n","\n","    Returns:\n","        Generated text response as string\n","    \"\"\"\n","    try:\n","        # Create config with only non-None parameters\n","        config_params = {}\n","        if temperature:\n","            config_params[\"temperature\"] = temperature\n","        if max_tokens is not None:\n","            config_params[\"max_output_tokens\"] = max_tokens\n","        if system_instructions:\n","            config_params[\"system_instruction\"] = system_instructions\n","\n","        # Create the config object\n","        config = types.GenerateContentConfig(**config_params)\n","\n","        # Generate content\n","        response = client.models.generate_content(\n","            model=model,\n","            contents=[prompt],\n","            config=config\n","        )\n","\n","        return response.text\n","    except Exception as e:\n","        return f\"Error generating text: {str(e)}\""],"metadata":{"id":"0SjRvwCSBK2b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BAPN7QDX9_Jm"},"source":["## 1. Basic Query Generation\n","\n","Let's start with the most basic approach to generating synthetic queries: simply asking an LLM to generate questions about the EU AI Act. This method requires minimal setup and can quickly produce a set of queries to serve as starting point.\n","\n","We'll use Gemini to generate some initial queries about the EU AI Act without providing any specific context from the actual documents.\n","\n","**Exercise 1a:**\n","> Write a short system prompt that tells Gemini more about the application we're building."]},{"cell_type":"code","metadata":{"id":"vkJ143MK9_Jm"},"source":["system_prompt = \"\"\"describe in 1-3 sentences what we're trying to achieve\"\"\""],"outputs":[],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"9MIU0yC_9_Jn"},"source":["**Exercise 1b:**\n","> Now write a simple prompt that asks the LLM to generate a question about the EU's AI Act. Then call Gemini with this prompt as well as the system prompt you wrote above. Note that you might need to add an instruction to tell it only to return the query and nothing else."]},{"cell_type":"code","metadata":{"id":"ERnej-j-9_Jn"},"source":["basic_prompt = \"\"\"write your prompt here\"\"\"\n","\n","# Generate the questions\n","response = ...\n","print(response)"],"outputs":[],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"Wn2PRWcP9_Jn"},"source":["**Exercise 1c:**\n","> Now run the same prompt 5 times"]},{"cell_type":"code","metadata":{"id":"iimjVgSE9_Jn"},"source":["..."],"outputs":[],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"-w_Fizna9_Jn"},"source":["**Reflection:**\n","> How would you evaluate the quality of these queries?"]},{"cell_type":"markdown","metadata":{"id":"rpqqwgJ39_Jo"},"source":["## 2. Grounded Generation\n","\n","To address the limitations we observed above, we'll now explore a more effective approach: grounding our query generation in actual passages from the EU AI Act.\n","\n","This technique significantly improves both the quality and diversity of our synthetic queries by:\n","1. Ensuring queries are relevant to the actual content of the document\n","2. Naturally increasing diversity as different passages cover different aspects of the legislation\n","3. Incorporating accurate terminology and concepts from the source material\n","\n","Let's load a few random passages from the AI Act."]},{"cell_type":"code","metadata":{"id":"Q573pLWx9_Jo"},"source":["import pandas as pd\n","df = pd.read_csv(\"sample_data.csv\")\n","len(df)"],"outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"id":"tm5jw9d49_Jo"},"source":["passages = df.sample(5, random_state=10)[\"passage\"].values.tolist()"],"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["Print out one passage which we'll use for grounded generation"],"metadata":{"id":"zSTTgdRwNvj5"}},{"cell_type":"code","metadata":{"id":"5cZWtWAz9_Jo"},"source":["passage = passages[0]\n","display(Markdown(passage))"],"outputs":[],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"ja8Svh_69_Jo"},"source":["In addition to the passage, we will also provide the LLM with a list of criteria which define a high quality query.\n","\n","**Exercise 2a:**\n","> Write a list of criteria which define a high quality query. For example, a high quality query should be specific, relevant and answerable by the passage."]},{"cell_type":"code","metadata":{"id":"_PInyody9_Jp"},"source":["criteria = \"\"\"\n","1. criterion 1\n","2. criterion 2\n","...\n","\"\"\""],"outputs":[],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"VbkoBV7Y9_Jp"},"source":["**Exercise 2b:**\n","> Write a prompt that asks the LLM to generate a query based on the provided criteria and passage.\n","\n","<details>\n","<summary>Click to see a hint</summary>\n","\n","Add the passage using a placeholder:\n","\n","```\n","\"\"\"\n","Write a query about the following passage:\n","\n","## Passage\n","\n","{passage}\n","\n","## Criteria\n","\n","Follow these criteria:\n","1. criterion 1\n","2. criterion 2\n","...\n","\"\"\"\n","```\n","\n","</details>"]},{"cell_type":"code","metadata":{"id":"sXToQBU89_Jp"},"source":["grounded_prompt = \"\"\"write your prompt here\"\"\""],"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["**Tip:**\n","> Always print out the fully formatted prompt to make sure everything is correct\n","\n","<details>\n","<summary>Click to see a hint</summary>\n","\n","Format the prompt by passing a passage to it:\n","\n","```\n","prompt = grounded_prompt.format(passage=passage)\n","```\n","\n","</details>"],"metadata":{"id":"ER983Qf-N52t"}},{"cell_type":"code","metadata":{"id":"oBJEsE-59_Jp"},"source":["prompt = ...\n","print(prompt)"],"outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"id":"AWMVKmeg9_Jp"},"source":["response = ...\n","print(response)"],"outputs":[],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"74GF27RR9_Jp"},"source":["**Exercise 2c:**\n","> Call Gemini 5 times with this prompt, each time using a different passage.\n","\n","**Tip:**\n","> *Look at your data!* Always make sure to print out the context used for grounding, along with the generated query. Only if you can see all the relevant information yourself will you be able to judge a query's quality."]},{"cell_type":"code","metadata":{"id":"r_q1NyMA9_Jx"},"source":["..."],"outputs":[],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"oE_gr2JV9_Jy"},"source":["**Reflection:**\n","> What do you think? Did this technique improve the generated questions?"]},{"cell_type":"markdown","metadata":{"id":"KyhWnQ149_Jy"},"source":["**Exercise 2d:**\n","> Now call Gemini 5 times with this prompt, each time using the same passage."]},{"cell_type":"code","metadata":{"id":"b7ZI--oq9_Jy"},"source":["..."],"outputs":[],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"QQ-pV9qI9_Jz"},"source":["## 3. Persona-Based Generation\n","\n","Our previous approach successfully grounded queries in relevant passages, but we still observed limited diversity in query styles and perspectives. To address this limitation, we'll now explore persona-based generation, a powerful technique for further enhancing the diversity of our synthetic queries.\n","\n","Persona-based generation involves creating detailed character profiles (personas) that guide the LLM to generate content from specific perspectives. This approach was introduced in the paper \"Scaling Synthetic Data Creation with 1,000,000,000 Personas\".\n","\n","The key benefits of persona-based generation for our query task include:\n","\n","1. **Stylistic diversity** - Different personas use different language patterns, terminology, and complexity levels\n","2. **Varied perspectives** - Personas with different backgrounds approach topics with unique concerns and priorities\n","3. **Realistic variation** - Real users come from diverse backgrounds and have different levels of domain knowledge\n","\n","Let's implement this approach by creating a set of personas with varying backgrounds, knowledge levels, and interests in the EU AI Act."]},{"cell_type":"markdown","metadata":{"id":"q52d4H8K9_Jz"},"source":["**Exercise 3a:**\n","> Create a list of 5 diverse personas who might have questions about the EU AI Act. Consider including different professions, technical backgrounds, and reasons for interest in the legislation."]},{"cell_type":"code","metadata":{"id":"XaV29c7Q9_J0"},"source":["personas = [\n","    \"A data protection officer at a large European enterprise implementing AI compliance\",\n","    \"A software developer specializing in machine learning applications\",\n","    ...\n","]"],"outputs":[],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"FWhAXVVL9_J0"},"source":["In addition to personas, we will also will generate a few different query styles to further increase diversity.\n","\n","**Exercise 3b:**\n","> Create a list of 5 different query styles. Try to make them as realistic and diverse as possible."]},{"cell_type":"code","metadata":{"id":"jwactKWS9_J0"},"source":["query_styles = [\n","    \"Technical language with domain-specific terminology\",\n","    \"Simple direct question with basic vocabulary\",\n","    ...\n","]"],"outputs":[],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"I6XEF6N59_J0"},"source":["**Exercise 3c:**\n","> Enhance our prompt from part 2 to also include persona and query style. Then call Gemini once with this prompt based on a single persona and query style.\n","\n","<details>\n","<summary>Click to see a hint</summary>\n","\n","Structure your prompt like this:\n","\n","```\n","\"\"\"\n","Write a query based on a passage, persona and query style.\n","\n","## Passage\n","\n","{passage}\n","\n","## Context:\n","\n","Persona: {persona}\n","Query style: {query_style}\n","\n","## Criteria\n","\n","Follow these criteria:\n","1. criterion 1\n","2. criterion 2\n","...\n","\"\"\"\n","```\n","\n","</details>"]},{"cell_type":"code","metadata":{"id":"SEzJIZ3a9_J0"},"source":["persona_prompt = \"\"\"write your prompt here\"\"\""],"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["<details>\n","<summary>Click to see a hint</summary>\n","\n","Pass all necessary inputs to your prompt:\n","\n","```\n","prompt = persona_prompt.format(\n","    passage=passage, persona=personas[0], query_style=query_styles[0]\n",")\n","```\n","\n","</details>"],"metadata":{"id":"FkmonHZjC7YE"}},{"cell_type":"code","metadata":{"id":"16Gr-aSX9_J0"},"source":["prompt = ...\n","print(prompt)"],"outputs":[],"execution_count":null},{"cell_type":"code","source":["response = ...\n","print(response)"],"metadata":{"id":"mcHgRt1qge9m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eap9obfN9_J0"},"source":["**Exercise 3d:**\n","> Run Gemini 5 times with this prompt on the same passage, but sample a different persona and query style each time you run it.\n","\n","<details>\n","<summary>Click to see a hint</summary>\n","\n","Sample a random item using `random.choice`:\n","\n","```\n","persona = random.choice(personas)\n","```"]},{"cell_type":"code","metadata":{"id":"jLaXZAm29_J1"},"source":["..."],"outputs":[],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"wPjOqTog9_J1"},"source":["**Reflect:**\n","> What do you think? Are we already happy with the diversity and quality of the queries?"]},{"cell_type":"markdown","metadata":{"id":"Z0FRWGVM9_J1"},"source":["## 4. Few-Shot Generation\n","\n","So far, we've made significant progress in generating diverse, relevant queries using passage grounding and persona-based techniques. Our queries are now much more varied in style, complexity, and perspective. However, we're still relying heavily on the LLM to interpret our instructions, personas, and query styles correctly.\n","\n","To gain more control over the generation process and further improve quality, we implement few-shot learning. This technique involves providing the LLM with carefully curated examples of exactly what we want it to produce and is frequently used in papers such as InPars (2022), Promptagator (2022), SWIM-IR (2024) and Gecko (2024).\n","\n","By showing the model high-quality examples that demonstrate the desired output format, style, and quality, we can:\n","1. **Increase consistency** - Examples provide concrete guidance on expected output format and quality\n","2. **Improve adherence to criteria** - Seeing examples helps the model better understand our quality criteria\n","3. **Reduce misinterpretations** - Examples clarify how personas and query styles should be applied\n","4. **Raise the quality bar** - Well-crafted examples set a higher standard for the generated queries\n","\n","Let's enhance our prompt with a few carefully selected examples that demonstrate the kind of high-quality, diverse queries we want to generate."]},{"cell_type":"markdown","metadata":{"id":"ZpGHX5jV9_J1"},"source":["**Load example data**\n","\n","Load the 4 few-shot examples we have prepared. For your own use case, you can either write few-shot examples by hand or generate them with the help of an LLM. However, make sure to review them carefully and if necessary, filter or improve them to ensure high quality.\n","\n","*Note that you need to provide not only example queries, but all additional inputs to the prompt, such as the passage, persona, and query style.*"]},{"cell_type":"code","metadata":{"id":"4672oL8E9_J1"},"source":["dfe = pd.read_csv(\"fewshot_examples.csv\")\n","dfe.head()"],"outputs":[],"execution_count":null},{"cell_type":"code","source":["def format_few_shot_examples(examples, k=None):\n","    \"\"\"\n","    Format a list of few-shot examples into a markdown-formatted string for prompts.\n","\n","    Args:\n","        examples: List of dictionaries containing 'passage', 'persona', 'query_style', and 'query'\n","        k: Optional number of examples to randomly sample (if None, use all examples)\n","\n","    Returns:\n","        A markdown-formatted string containing the examples\n","    \"\"\"\n","    # If k is specified, randomly sample k examples\n","    if k is not None and k < len(examples):\n","        examples = random.sample(list(examples), k)\n","\n","    formatted_examples = []\n","    for i, example in enumerate(examples):\n","        example_text = f\"## Example {i+1}\\n\\n\"\n","        example_text += f\"### Passage:\\n\\n{example['passage']}\\n\\n\"\n","        example_text += f\"### Context:\\n\\nPersona: {example['persona']}\\nQuery style: {example['query_style']}\\n\\n\"\n","        example_text += f\"### Generated Query: {example['query']}\"\n","        formatted_examples.append(example_text)\n","\n","    return \"\\n\\n\".join(formatted_examples)"],"metadata":{"id":"S3AckaEXNM8D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We format our few-shot examples as a single string using the function above."],"metadata":{"id":"769mDxvUDrll"}},{"cell_type":"code","source":["examples = dfe.to_dict(orient=\"records\")\n","examples_formatted = format_few_shot_examples(examples)\n","print(examples_formatted)"],"metadata":{"id":"5VQXj8q0Dqm2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ysGceyFX9_J1"},"source":["**Exercise 4a:**\n","> Enhance our prompt from 3c to also include few shot examples. Print out the full final prompt and then call Gemini once with this prompt.\n","\n","**Tip:**\n","> You can further increase the diversity of the data generation process by randomly sampling a subset of examples."]},{"cell_type":"markdown","source":["<details>\n","<summary>Click to see a hint</summary>\n","\n","Structure your prompt like this:\n","\n","```\n","\"\"\"\n","Write a query based on a passage, persona and query style.\n","\n","# Criteria:\n","\n","Follow these criteria\n","1. criterion 1\n","2. criterion 2\n","...\n","\n","# Examples\n","\n","{examples_formatted}\n","\n","# Your task\n","\n","Now generate a query about the following passage.\n","\n","### Passage:\n","\n","{passage}\n","\n","### Context:\n","\n","Persona: {persona}\n","Query style: {query_style}\n","\n","### Generated Query:\"\"\"\n","```"],"metadata":{"id":"-PycHcbnADMg"}},{"cell_type":"code","metadata":{"id":"U1dKPXpq9_J2"},"source":["fewshot_prompt = \"\"\"write your prompt here\"\"\""],"outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"id":"4r7ENM9l9_J2"},"source":["prompt = ...\n","print(prompt)"],"outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"id":"0s3yLKYh9_J2"},"source":["response = ...\n","print(response)"],"outputs":[],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"RvdIPJoe9_J2"},"source":["**Exercise 4b:**\n","> As before, run 5 iterations with the same passage, and sample a different persona and query style each time you run it. However, at each iteration call both `persona_prompt` and `fewshot_prompt` so that we're able to directly compare them. Also, store the generated queries, along with their sampled personas and query styles in a list, as we'll use them in the next section."]},{"cell_type":"code","metadata":{"id":"SC7R_dv39_J2"},"source":["results = []\n","\n","print(\"# PASSAGE:\\n\")\n","display(Markdown(passage))\n","print(\"\\n\" + \"-\"*80 + \"\\n\")\n","\n","for i in range(5):\n","    # Sample a random persona and query style\n","    ...\n","\n","    # Format the prompts with the same passage, persona, and query style\n","    ...\n","\n","    # Generate responses from both prompts\n","    ...\n","    time.sleep(0.5)  # Avoid overloading the API\n","\n","    # Store results\n","    results.append({\n","        \"persona\": ...,\n","        \"query_style\": ...,\n","        \"persona_query\": ...,\n","        \"fewshot_query\": ...\n","    })\n","\n","    # Print results for comparison\n","    ..."],"outputs":[],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"AJE5CM489_J2"},"source":["**Reflect:**\n","> What do you notice about the queries generated by the two prompts? Which one do you think is better?"]},{"cell_type":"markdown","metadata":{"id":"KyqCqe-k9_J3"},"source":["## 5. Quality Filtering\n","\n","We've now explored several techniques for generating diverse, high-quality synthetic queries. By combining passage grounding, persona-based generation, and few-shot examples, we've significantly improved both the quality and diversity of our synthetic data. However, even with these advanced techniques, not every generated query will meet our standards.\n","\n","A crucial final step in synthetic data generation is quality filtering. As highlighted in the Answer.AI blog:\n","\n","> \"To address these concerns, let’s use another prompt. It will evaluate and filter the generations. We’ll use the 5-point scoring system in The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale. It proved most effective at evaluating the quality of data.\"\n","\n","The FineWeb paper introduced an additive scoring approach where points are accumulated based on satisfying specific quality criteria. The LLM is instructed to first write a critique of the generated example and then score it based on the provided scoring system."]},{"cell_type":"markdown","metadata":{"id":"mfazNnx79_J3"},"source":["Here is the prompt used in the blog post:"]},{"cell_type":"code","metadata":{"id":"U-CHcp2-9_J3"},"source":["eval_prompt_template = \"\"\"\n","Below is an extract of a translation. Evaluate its quality as a senior translator would, considering its suitability for professional use. Use the additive 5-point scoring system described below. Points are accumulated based on the satisfaction of each criterion:\n","\n","- Add 1 point if the translation conveys the basic meaning of the source text, even if it includes some minor errors or awkward phrasing.\n","- Add another point if the translation is generally accurate but lacks refinement in style or fails to capture some nuances of the original. It might use inconsistent terminology or have occasional lapses in register.\n","- Award a third point if the translation is appropriate for professional use and accurately conveys key concepts of the source text. It demonstrates good understanding of both languages, though it may not be flawless or could include some slight inconsistencies. It resembles the work of a competent translator but may have room for improvement in fluency or precision.\n","- Grant a fourth point if the translation is highly accurate and reads naturally in the target language, exhibiting a consistent and appropriate style. It could be similar to the work of an experienced translator, offering faithful rendering of content and tone, with minimal errors, and effectively handling complex concepts or cultural references. The result is coherent, well-expressed, and valuable for its intended purpose.\n","- Bestow a fifth point if the translation is outstanding, demonstrating mastery of both source and target languages. It captures subtle nuances, maintains the author's voice and intent, and reads as if it were originally written in the target language. The translator has made excellent choices in dealing with challenging elements like wordplay, idiomatic expressions, or culture-specific content.\n","\n","<translation>\n","{translation}\n","</translation>\n","\n","After examining the translation:\n","\n","- Briefly justify your total score in a single line.\n","- Conclude with the score of the translation.\"\"\""],"outputs":[],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"cMCSbBIZ9_J3"},"source":["**Exercise 5a:**\n","> Rewrite the scoring instructions above to fit our use case. Reuse the quality criteria you have written before. We already prefilled all the rest for you.\n","\n","**Tip:**\n","> Asking the model to return its reponse in JSON format will simplify parsing and postprocessing."]},{"cell_type":"code","metadata":{"id":"SWTyy5Of9_J3"},"source":["quality_filter_prompt = \"\"\"\n","Write your scoring instructions here\n","\n","## Passage:\n","\n","{passage}\n","\n","## Context:\n","Persona: {persona}\n","Query style: {query_style}\n","\n","## Generated Query: {query}\n","\n","After examining the query:\n","\n","- Briefly justify your total score in a single line.\n","- Conclude with the score of the query (1-5).\n","\n","Return the evaluation in valid JSON format with double quotes:\n","\n","Evaluation = {{\"critique\": str, \"score\": int}}\n","Return: Evaluation\n","\"\"\""],"outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"id":"q1vaYM1E9_J3"},"source":["prompt = ...\n","print(prompt)"],"outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"id":"EyuDTMQT9_J3"},"source":["response = ...\n","print(response)"],"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["We can use this function to parse the response"],"metadata":{"id":"bH2oePomOGoO"}},{"cell_type":"code","source":["import json\n","\n","def extract_json(response_text):\n","    \"\"\"Extract and parse JSON from LLM response text, handling various formats.\"\"\"\n","    try:\n","        # First try to extract JSON from markdown code blocks if present\n","        if \"```\" in response_text:\n","            # Extract content between code blocks\n","            json_text = response_text.split(\"```\")[1]\n","            # Remove language indicator if present\n","            if json_text.startswith(\"json\"):\n","                json_text = json_text[4:].strip()\n","        else:\n","            json_text = response_text.strip()\n","\n","        # Parse the JSON\n","        return json.loads(json_text)\n","    except (json.JSONDecodeError, IndexError) as e:\n","        print(f\"Error parsing JSON: {e}\")\n","        print(f\"Raw response: {response_text}\")\n","        # Return a default value in case of error\n","        return {\"critique\": \"Error parsing response\", \"score\": 0}"],"metadata":{"id":"Z6RYxaglOF-F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["extract_json(response)"],"metadata":{"id":"LGkXefC7crnj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bAKs0WAh9_J3"},"source":["**Exercise 5b:**\n","> Now run the prompt on all results you stored in the previous section, parse the responses and store the critiques and scores together with the previous results. To simplify things we already wrote this part for you."]},{"cell_type":"code","metadata":{"id":"G_pqGeUr9_J4"},"source":["# Print the passage so we can easily check it\n","print(\"# PASSAGE:\\n\")\n","display(Markdown(passage))\n","print(\"\\n\" + \"-\"*80 + \"\\n\")\n","\n","# Evaluate all queries and store results\n","for i, result in enumerate(results):\n","    print(f\"Evaluating query {i+1}/5...\\n\")\n","\n","    # Evaluate persona-based query\n","    persona_filter_prompt = quality_filter_prompt.format(\n","        passage=passage,\n","        persona=result[\"persona\"],\n","        query_style=result[\"query_style\"],\n","        query=result[\"persona_query\"]\n","    )\n","    persona_response = generate_text(persona_filter_prompt, system_instructions=system_prompt)\n","\n","    # Evaluate few-shot query\n","    fewshot_filter_prompt = quality_filter_prompt.format(\n","        passage=passage,\n","        persona=result[\"persona\"],\n","        query_style=result[\"query_style\"],\n","        query=result[\"fewshot_query\"]\n","    )\n","    fewshot_response = generate_text(fewshot_filter_prompt, system_instructions=system_prompt)\n","    time.sleep(0.5)\n","\n","    # Parse and store evaluations\n","    try:\n","        persona_eval = extract_json(persona_response)\n","        fewshot_eval = extract_json(fewshot_response)\n","\n","        # Add evaluations to results\n","        results[i][\"persona_critique\"] = persona_eval.get(\"critique\", \"Error\")\n","        results[i][\"persona_score\"] = persona_eval.get(\"score\", 0)\n","        results[i][\"fewshot_critique\"] = fewshot_eval.get(\"critique\", \"Error\")\n","        results[i][\"fewshot_score\"] = fewshot_eval.get(\"score\", 0)\n","\n","        # Print personas and query styles\n","        print(f\"  PERSONA: {results[i]['persona']}\")\n","        print(f\"  QUERY STYLE: {results[i]['query_style']}\")\n","\n","        # Print scores during generation\n","        print(f\"\\n  ZERO-SHOT SCORE: {results[i]['persona_score']}\")\n","        print(f\"     Query: {results[i]['persona_query'].strip()}\")\n","        print(f\"     Critique: {results[i]['persona_critique'].strip()}\")\n","        print(f\"\\n  FEW-SHOT SCORE: {results[i]['fewshot_score']}\")\n","        print(f\"     Query: {results[i]['fewshot_query'].strip()}\")\n","        print(f\"     Critique: {results[i]['fewshot_critique'].strip()}\")\n","\n","    except Exception as e:\n","        print(f\"Error processing result {i}: {e}\")\n","\n","    print(\"\\n\" + \"-\"*80 + \"\\n\")\n","\n","# Calculate average scores\n","avg_persona_score = sum(r.get(\"persona_score\", 0) for r in results) / len(results)\n","avg_fewshot_score = sum(r.get(\"fewshot_score\", 0) for r in results) / len(results)\n","\n","print(\"=\" * 80)\n","print(\"AVERAGE SCORES:\")\n","print(f\"Zero-shot queries: {avg_persona_score:.2f}\")\n","print(f\"Few-shot queries: {avg_fewshot_score:.2f}\")\n","print(\"=\" * 80)"],"outputs":[],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"szch5joi9_J5"},"source":["## Conclusion\n","\n","In this notebook, we've explored a comprehensive approach to synthetic query generation for specialized domains like the EU AI Act. We've progressed from basic generation to increasingly sophisticated techniques:\n","\n","1. **Basic generation** demonstrated the limitations of simple prompting\n","2. **Grounded generation** improved relevance by anchoring queries to specific passages\n","3. **Persona-based generation** enhanced diversity through varied perspectives and styles\n","4. **Few-shot learning** provided more control over output quality and format\n","5. **Quality filtering** ensured only the best queries make it into our final dataset\n","\n","These techniques allow us to create synthetic queries that are both high-quality and diverse.\n","\n","However, generating good queries is only the first step in creating a robust training dataset for embedding models. In the next notebook, we'll explore the crucial process of mining positive and negative examples - identifying which passages truly answer our queries and which ones don't. This step is essential for creating the clean, well-structured training data needed for effective embedding model fine-tuning."]}],"metadata":{"kernelspec":{"display_name":"python3","language":"python","name":"python3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}